{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# IFT3395 Competition 2 - Milestone 2 v2\n",
                "\n",
                "## 目标 Goal\n",
                "提高Kaggle分数至 > 0.525。\n",
                "\n",
                "## 策略 Strategy\n",
                "1. **自定义CNN (From Scratch)** - 轻量级ResNet风格网络\n",
                "2. **处理类别不平衡** - WeightedRandomSampler + class_weight\n",
                "3. **集成学习** - CNN + ExtraTrees + RandomForest\n",
                "4. **数据增强** - 翻转、旋转、颜色抖动\n",
                "5. **测试时增强 (TTA)** - 提高泛化能力"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Using device: cpu\n"
                    ]
                }
            ],
            "source": [
                "# ==================== Cell 1: Imports ====================\n",
                "import pickle\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from pathlib import Path\n",
                "import random\n",
                "import os\n",
                "import warnings\n",
                "from PIL import Image\n",
                "\n",
                "# PyTorch\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
                "from torchvision import transforms\n",
                "\n",
                "# Sklearn\n",
                "from sklearn.model_selection import StratifiedKFold\n",
                "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.metrics import accuracy_score\n",
                "\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# 设定随机种子\n",
                "SEED = 42\n",
                "def seed_everything(seed):\n",
                "    random.seed(seed)\n",
                "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
                "    np.random.seed(seed)\n",
                "    torch.manual_seed(seed)\n",
                "    if torch.cuda.is_available():\n",
                "        torch.cuda.manual_seed(seed)\n",
                "        torch.backends.cudnn.deterministic = True\n",
                "        torch.backends.cudnn.benchmark = False\n",
                "\n",
                "seed_everything(SEED)\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Train Shape: (1080, 28, 28, 3)\n",
                        "Test Shape: (400, 28, 28, 3)\n",
                        "Class Counts: [486 128 206 194  66]\n"
                    ]
                }
            ],
            "source": [
                "# ==================== Cell 2: Load Data ====================\n",
                "DATA_DIR = Path('data')\n",
                "\n",
                "def load_data():\n",
                "    with open(DATA_DIR / 'train_data.pkl', 'rb') as f:\n",
                "        train = pickle.load(f)\n",
                "    with open(DATA_DIR / 'test_data.pkl', 'rb') as f:\n",
                "        test = pickle.load(f)\n",
                "    return train, test\n",
                "\n",
                "train_data, test_data = load_data()\n",
                "\n",
                "X_train_raw = train_data['images']\n",
                "y_train = train_data['labels'].flatten().astype(np.int64)\n",
                "X_test_raw = test_data['images']\n",
                "\n",
                "# 确保是 uint8 (0-255)\n",
                "if X_train_raw.max() <= 1.0:\n",
                "    X_train_raw = (X_train_raw * 255).astype(np.uint8)\n",
                "    X_test_raw = (X_test_raw * 255).astype(np.uint8)\n",
                "else:\n",
                "    X_train_raw = X_train_raw.astype(np.uint8)\n",
                "    X_test_raw = X_test_raw.astype(np.uint8)\n",
                "\n",
                "print(\"Train Shape:\", X_train_raw.shape)\n",
                "print(\"Test Shape:\", X_test_raw.shape)\n",
                "print(\"Class Counts:\", np.bincount(y_train))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Datasets and transforms defined.\n"
                    ]
                }
            ],
            "source": [
                "# ==================== Cell 3: Dataset & Transforms ====================\n",
                "class IFT3395Dataset(Dataset):\n",
                "    \"\"\"Custom Dataset for labeled and unlabeled data.\"\"\"\n",
                "    def __init__(self, images, labels=None, transform=None):\n",
                "        self.images = images\n",
                "        self.labels = labels\n",
                "        self.transform = transform\n",
                "        \n",
                "    def __len__(self):\n",
                "        return len(self.images)\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        img_arr = self.images[idx].astype(np.uint8)\n",
                "        img = Image.fromarray(img_arr)\n",
                "        \n",
                "        if self.transform:\n",
                "            img = self.transform(img)\n",
                "            \n",
                "        if self.labels is not None:\n",
                "            return img, torch.tensor(self.labels[idx], dtype=torch.long)\n",
                "        return img  # No label for test set\n",
                "\n",
                "# 训练时增强\n",
                "train_transform = transforms.Compose([\n",
                "    transforms.RandomHorizontalFlip(p=0.5),\n",
                "    transforms.RandomRotation(15),\n",
                "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
                "])\n",
                "\n",
                "# 验证/测试时不增强\n",
                "val_transform = transforms.Compose([\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
                "])\n",
                "\n",
                "# TTA: 水平翻转\n",
                "tta_flip_transform = transforms.Compose([\n",
                "    transforms.RandomHorizontalFlip(p=1.0),  # Always flip\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
                "])\n",
                "\n",
                "print(\"Datasets and transforms defined.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ResNet9 model defined.\n"
                    ]
                }
            ],
            "source": [
                "# ==================== Cell 4: CNN Model ====================\n",
                "def conv_bn(in_ch, out_ch, pool=False):\n",
                "    layers = [\n",
                "        nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1, bias=False),\n",
                "        nn.BatchNorm2d(out_ch),\n",
                "        nn.ReLU(inplace=True)\n",
                "    ]\n",
                "    if pool:\n",
                "        layers.append(nn.MaxPool2d(2))\n",
                "    return nn.Sequential(*layers)\n",
                "\n",
                "class ResNet9(nn.Module):\n",
                "    \"\"\"Lightweight ResNet-9 for small images (28x28).\"\"\"\n",
                "    def __init__(self, num_classes=5):\n",
                "        super().__init__()\n",
                "        self.prep = conv_bn(3, 64)\n",
                "        \n",
                "        self.layer1 = conv_bn(64, 128, pool=True)\n",
                "        self.res1 = nn.Sequential(conv_bn(128, 128), conv_bn(128, 128))\n",
                "        \n",
                "        self.layer2 = conv_bn(128, 256, pool=True)\n",
                "        \n",
                "        self.layer3 = conv_bn(256, 512, pool=True)\n",
                "        self.res2 = nn.Sequential(conv_bn(512, 512), conv_bn(512, 512))\n",
                "        \n",
                "        self.classifier = nn.Sequential(\n",
                "            nn.AdaptiveMaxPool2d(1),\n",
                "            nn.Flatten(),\n",
                "            nn.Dropout(0.3),\n",
                "            nn.Linear(512, num_classes)\n",
                "        )\n",
                "        \n",
                "    def forward(self, x):\n",
                "        x = self.prep(x)\n",
                "        x = self.layer1(x)\n",
                "        x = x + self.res1(x)\n",
                "        x = self.layer2(x)\n",
                "        x = self.layer3(x)\n",
                "        x = x + self.res2(x)\n",
                "        x = self.classifier(x)\n",
                "        return x\n",
                "\n",
                "print(\"ResNet9 model defined.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Helper functions defined.\n"
                    ]
                }
            ],
            "source": [
                "# ==================== Cell 5: Helper Functions ====================\n",
                "def get_sampler(labels):\n",
                "    \"\"\"Create WeightedRandomSampler for class imbalance.\"\"\"\n",
                "    class_counts = np.bincount(labels)\n",
                "    class_counts = np.maximum(class_counts, 1)  # Avoid division by zero\n",
                "    class_weights = 1.0 / class_counts\n",
                "    sample_weights = class_weights[labels]\n",
                "    return WeightedRandomSampler(\n",
                "        torch.from_numpy(sample_weights).double(), \n",
                "        len(sample_weights)\n",
                "    )\n",
                "\n",
                "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
                "    \"\"\"Train for one epoch.\"\"\"\n",
                "    model.train()\n",
                "    total_loss, correct, total = 0.0, 0, 0\n",
                "    \n",
                "    for imgs, labels in loader:\n",
                "        imgs, labels = imgs.to(device), labels.to(device)\n",
                "        optimizer.zero_grad()\n",
                "        outputs = model(imgs)\n",
                "        loss = criterion(outputs, labels)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        \n",
                "        total_loss += loss.item() * imgs.size(0)\n",
                "        _, preds = outputs.max(1)\n",
                "        total += labels.size(0)\n",
                "        correct += preds.eq(labels).sum().item()\n",
                "        \n",
                "    return total_loss / total, correct / total\n",
                "\n",
                "@torch.no_grad()\n",
                "def validate(model, loader, criterion, device):\n",
                "    \"\"\"Validate with labels.\"\"\"\n",
                "    model.eval()\n",
                "    total_loss, correct, total = 0.0, 0, 0\n",
                "    all_probs = []\n",
                "    \n",
                "    for imgs, labels in loader:\n",
                "        imgs, labels = imgs.to(device), labels.to(device)\n",
                "        outputs = model(imgs)\n",
                "        loss = criterion(outputs, labels)\n",
                "        \n",
                "        total_loss += loss.item() * imgs.size(0)\n",
                "        _, preds = outputs.max(1)\n",
                "        total += labels.size(0)\n",
                "        correct += preds.eq(labels).sum().item()\n",
                "        all_probs.append(F.softmax(outputs, dim=1).cpu().numpy())\n",
                "        \n",
                "    return total_loss / total, correct / total, np.concatenate(all_probs)\n",
                "\n",
                "@torch.no_grad()\n",
                "def predict(model, loader, device):\n",
                "    \"\"\"Predict on unlabeled test data.\"\"\"\n",
                "    model.eval()\n",
                "    all_probs = []\n",
                "    \n",
                "    for imgs in loader:\n",
                "        imgs = imgs.to(device)\n",
                "        outputs = model(imgs)\n",
                "        all_probs.append(F.softmax(outputs, dim=1).cpu().numpy())\n",
                "        \n",
                "    return np.concatenate(all_probs)\n",
                "\n",
                "print(\"Helper functions defined.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Starting 5-Fold Cross Validation...\n",
                        "Epochs: 50, Batch Size: 32, LR: 0.001\n",
                        "============================================================\n"
                    ]
                }
            ],
            "source": [
                "# ==================== Cell 6: Training with K-Fold CV ====================\n",
                "N_FOLDS = 5\n",
                "EPOCHS = 50\n",
                "BATCH_SIZE = 32\n",
                "LR = 0.001\n",
                "\n",
                "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
                "\n",
                "# Storage for predictions\n",
                "cnn_oof_probs = np.zeros((len(X_train_raw), 5))\n",
                "cnn_test_probs = np.zeros((len(X_test_raw), 5))\n",
                "\n",
                "# Sklearn models storage\n",
                "et_oof_probs = np.zeros((len(X_train_raw), 5))\n",
                "et_test_probs = np.zeros((len(X_test_raw), 5))\n",
                "rf_oof_probs = np.zeros((len(X_train_raw), 5))\n",
                "rf_test_probs = np.zeros((len(X_test_raw), 5))\n",
                "\n",
                "# Flatten data for sklearn\n",
                "X_train_flat = X_train_raw.reshape(len(X_train_raw), -1).astype(np.float32) / 255.0\n",
                "X_test_flat = X_test_raw.reshape(len(X_test_raw), -1).astype(np.float32) / 255.0\n",
                "\n",
                "print(f\"Starting {N_FOLDS}-Fold Cross Validation...\")\n",
                "print(f\"Epochs: {EPOCHS}, Batch Size: {BATCH_SIZE}, LR: {LR}\")\n",
                "print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "==================== Fold 1/5 ====================\n",
                        "  Epoch 10 | Train Acc: 0.341 | Val Acc: 0.301\n",
                        "  Epoch 20 | Train Acc: 0.428 | Val Acc: 0.370\n",
                        "  Epoch 30 | Train Acc: 0.505 | Val Acc: 0.509\n",
                        "  Epoch 40 | Train Acc: 0.602 | Val Acc: 0.417\n",
                        "  Epoch 50 | Train Acc: 0.624 | Val Acc: 0.486\n",
                        "  [CNN] Best Val Acc: 0.5093\n",
                        "  [ET]  Val Acc: 0.4954\n",
                        "  [RF]  Val Acc: 0.5139\n",
                        "\n",
                        "==================== Fold 2/5 ====================\n",
                        "  Epoch 10 | Train Acc: 0.366 | Val Acc: 0.333\n",
                        "  Epoch 20 | Train Acc: 0.394 | Val Acc: 0.389\n",
                        "  Epoch 30 | Train Acc: 0.458 | Val Acc: 0.435\n",
                        "  Epoch 40 | Train Acc: 0.567 | Val Acc: 0.421\n",
                        "  Epoch 50 | Train Acc: 0.598 | Val Acc: 0.440\n",
                        "  [CNN] Best Val Acc: 0.4630\n",
                        "  [ET]  Val Acc: 0.5370\n",
                        "  [RF]  Val Acc: 0.5185\n",
                        "\n",
                        "==================== Fold 3/5 ====================\n",
                        "  Epoch 10 | Train Acc: 0.372 | Val Acc: 0.403\n",
                        "  Epoch 20 | Train Acc: 0.442 | Val Acc: 0.389\n",
                        "  Epoch 30 | Train Acc: 0.502 | Val Acc: 0.481\n",
                        "  Epoch 40 | Train Acc: 0.596 | Val Acc: 0.477\n",
                        "  Epoch 50 | Train Acc: 0.645 | Val Acc: 0.468\n",
                        "  [CNN] Best Val Acc: 0.5000\n",
                        "  [ET]  Val Acc: 0.5139\n",
                        "  [RF]  Val Acc: 0.5139\n",
                        "\n",
                        "==================== Fold 4/5 ====================\n",
                        "  Epoch 10 | Train Acc: 0.358 | Val Acc: 0.403\n"
                    ]
                },
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     26\u001b[39m best_weights = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     t_loss, t_acc = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m     v_loss, v_acc, _ = validate(model, val_loader, criterion, device)\n\u001b[32m     31\u001b[39m     scheduler.step()\n",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, loader, criterion, optimizer, device)\u001b[39m\n\u001b[32m     21\u001b[39m outputs = model(imgs)\n\u001b[32m     22\u001b[39m loss = criterion(outputs, labels)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m optimizer.step()\n\u001b[32m     26\u001b[39m total_loss += loss.item() * imgs.size(\u001b[32m0\u001b[39m)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\torch\\_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\torch\\autograd\\graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
                        "\u001b[31mKeyboardInterrupt\u001b[39m: "
                    ]
                }
            ],
            "source": [
                "# ==================== Cell 7: Main Training Loop ====================\n",
                "fold_scores = []\n",
                "\n",
                "for fold, (train_idx, val_idx) in enumerate(skf.split(X_train_raw, y_train)):\n",
                "    print(f\"\\n{'='*20} Fold {fold+1}/{N_FOLDS} {'='*20}\")\n",
                "    \n",
                "    # Split data\n",
                "    X_tr, y_tr = X_train_raw[train_idx], y_train[train_idx]\n",
                "    X_val, y_val = X_train_raw[val_idx], y_train[val_idx]\n",
                "    \n",
                "    # ========== CNN Training ==========\n",
                "    sampler = get_sampler(y_tr)\n",
                "    train_ds = IFT3395Dataset(X_tr, y_tr, transform=train_transform)\n",
                "    val_ds = IFT3395Dataset(X_val, y_val, transform=val_transform)\n",
                "    \n",
                "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=sampler, num_workers=0)\n",
                "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
                "    \n",
                "    # Model, loss, optimizer\n",
                "    model = ResNet9(num_classes=5).to(device)\n",
                "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
                "    optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
                "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
                "    \n",
                "    best_acc = 0.0\n",
                "    best_weights = None\n",
                "    \n",
                "    for epoch in range(EPOCHS):\n",
                "        t_loss, t_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
                "        v_loss, v_acc, _ = validate(model, val_loader, criterion, device)\n",
                "        scheduler.step()\n",
                "        \n",
                "        if v_acc > best_acc:\n",
                "            best_acc = v_acc\n",
                "            best_weights = model.state_dict().copy()\n",
                "            \n",
                "        if (epoch + 1) % 10 == 0:\n",
                "            print(f\"  Epoch {epoch+1:02d} | Train Acc: {t_acc:.3f} | Val Acc: {v_acc:.3f}\")\n",
                "    \n",
                "    # Load best model\n",
                "    model.load_state_dict(best_weights)\n",
                "    _, _, val_probs = validate(model, val_loader, criterion, device)\n",
                "    cnn_oof_probs[val_idx] = val_probs\n",
                "    print(f\"  [CNN] Best Val Acc: {best_acc:.4f}\")\n",
                "    \n",
                "    # TTA for test set: Original + Flip\n",
                "    test_ds_orig = IFT3395Dataset(X_test_raw, labels=None, transform=val_transform)\n",
                "    test_ds_flip = IFT3395Dataset(X_test_raw, labels=None, transform=tta_flip_transform)\n",
                "    \n",
                "    test_loader_orig = DataLoader(test_ds_orig, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
                "    test_loader_flip = DataLoader(test_ds_flip, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
                "    \n",
                "    probs_orig = predict(model, test_loader_orig, device)\n",
                "    probs_flip = predict(model, test_loader_flip, device)\n",
                "    \n",
                "    # Average TTA predictions\n",
                "    cnn_test_probs += (probs_orig + probs_flip) / 2 / N_FOLDS\n",
                "    \n",
                "    # ========== ExtraTrees ==========\n",
                "    et = ExtraTreesClassifier(\n",
                "        n_estimators=500,\n",
                "        max_depth=None,\n",
                "        min_samples_split=2,\n",
                "        random_state=SEED + fold,\n",
                "        class_weight='balanced',\n",
                "        n_jobs=-1\n",
                "    )\n",
                "    et.fit(X_train_flat[train_idx], y_train[train_idx])\n",
                "    et_val_acc = et.score(X_train_flat[val_idx], y_train[val_idx])\n",
                "    et_oof_probs[val_idx] = et.predict_proba(X_train_flat[val_idx])\n",
                "    et_test_probs += et.predict_proba(X_test_flat) / N_FOLDS\n",
                "    print(f\"  [ET]  Val Acc: {et_val_acc:.4f}\")\n",
                "    \n",
                "    # ========== RandomForest ==========\n",
                "    rf = RandomForestClassifier(\n",
                "        n_estimators=500,\n",
                "        max_depth=None,\n",
                "        min_samples_split=2,\n",
                "        random_state=SEED + fold,\n",
                "        class_weight='balanced',\n",
                "        n_jobs=-1\n",
                "    )\n",
                "    rf.fit(X_train_flat[train_idx], y_train[train_idx])\n",
                "    rf_val_acc = rf.score(X_train_flat[val_idx], y_train[val_idx])\n",
                "    rf_oof_probs[val_idx] = rf.predict_proba(X_train_flat[val_idx])\n",
                "    rf_test_probs += rf.predict_proba(X_test_flat) / N_FOLDS\n",
                "    print(f\"  [RF]  Val Acc: {rf_val_acc:.4f}\")\n",
                "    \n",
                "    fold_scores.append({'cnn': best_acc, 'et': et_val_acc, 'rf': rf_val_acc})\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"Training Complete!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ==================== Cell 8: Ensemble & Evaluation ====================\n",
                "# Calculate overall CV accuracy for each model\n",
                "cnn_cv_acc = accuracy_score(y_train, np.argmax(cnn_oof_probs, axis=1))\n",
                "et_cv_acc = accuracy_score(y_train, np.argmax(et_oof_probs, axis=1))\n",
                "rf_cv_acc = accuracy_score(y_train, np.argmax(rf_oof_probs, axis=1))\n",
                "\n",
                "print(\"=\" * 50)\n",
                "print(\"Cross-Validation Results:\")\n",
                "print(f\"  CNN OOF Accuracy:        {cnn_cv_acc:.4f}\")\n",
                "print(f\"  ExtraTrees OOF Accuracy: {et_cv_acc:.4f}\")\n",
                "print(f\"  RandomForest OOF Accuracy: {rf_cv_acc:.4f}\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "# Try different ensemble weights\n",
                "best_ensemble_acc = 0\n",
                "best_weights = None\n",
                "\n",
                "for w_cnn in np.arange(0.2, 0.8, 0.1):\n",
                "    for w_et in np.arange(0.1, 0.6, 0.1):\n",
                "        w_rf = 1.0 - w_cnn - w_et\n",
                "        if w_rf < 0:\n",
                "            continue\n",
                "        \n",
                "        oof_probs = w_cnn * cnn_oof_probs + w_et * et_oof_probs + w_rf * rf_oof_probs\n",
                "        acc = accuracy_score(y_train, np.argmax(oof_probs, axis=1))\n",
                "        \n",
                "        if acc > best_ensemble_acc:\n",
                "            best_ensemble_acc = acc\n",
                "            best_weights = (w_cnn, w_et, w_rf)\n",
                "\n",
                "print(f\"\\nBest Ensemble Weights: CNN={best_weights[0]:.1f}, ET={best_weights[1]:.1f}, RF={best_weights[2]:.1f}\")\n",
                "print(f\"Best Ensemble OOF Accuracy: {best_ensemble_acc:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ==================== Cell 9: Generate Submission ====================\n",
                "# Use best weights for final prediction\n",
                "w_cnn, w_et, w_rf = best_weights\n",
                "final_test_probs = w_cnn * cnn_test_probs + w_et * et_test_probs + w_rf * rf_test_probs\n",
                "predictions = np.argmax(final_test_probs, axis=1)\n",
                "\n",
                "# Create submission\n",
                "submission = pd.DataFrame({\n",
                "    'ImageId': np.arange(len(predictions)),\n",
                "    'Label': predictions\n",
                "})\n",
                "\n",
                "submission.to_csv('submission_milestone2_v2.csv', index=False)\n",
                "print(\"\\nSubmission saved to 'submission_milestone2_v2.csv'\")\n",
                "print(f\"\\nPrediction Distribution:\")\n",
                "print(submission['Label'].value_counts().sort_index())\n",
                "\n",
                "# Sanity check\n",
                "print(f\"\\nTotal Predictions: {len(predictions)}\")\n",
                "print(f\"Expected: {len(X_test_raw)}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.14.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
