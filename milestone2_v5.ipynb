{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# IFT3395 Competition 2 - Milestone 2 v5\n",
                "\n",
                "## 目标: Validation Accuracy > 0.53, 训练时间 < 15分钟\n",
                "\n",
                "## v5改进:\n",
                "- **更宽的CNN**: 通道数 64->128->256->512\n",
                "- **RandomErasing**: 增强泛化\n",
                "- **TTA (Test-Time Augmentation)**: 测试时多次预测取平均\n",
                "- **更多特征**: 添加梯度直方图特征\n",
                "- **精细权重搜索**: 0.05步长"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Device: cpu\n"
                    ]
                }
            ],
            "source": [
                "# Cell 1: Imports\n",
                "import pickle\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from pathlib import Path\n",
                "import random, os, warnings, time\n",
                "from PIL import Image\n",
                "\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
                "from torchvision import transforms\n",
                "\n",
                "from sklearn.model_selection import StratifiedKFold\n",
                "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, HistGradientBoostingClassifier, GradientBoostingClassifier\n",
                "from sklearn.metrics import accuracy_score\n",
                "\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "SEED = 42\n",
                "def seed_all(seed):\n",
                "    random.seed(seed)\n",
                "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
                "    np.random.seed(seed)\n",
                "    torch.manual_seed(seed)\n",
                "    if torch.cuda.is_available():\n",
                "        torch.cuda.manual_seed(seed)\n",
                "        torch.backends.cudnn.deterministic = True\n",
                "\n",
                "seed_all(SEED)\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Device: {device}\")\n",
                "t0 = time.time()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Train: (1080, 28, 28, 3), Test: (400, 28, 28, 3)\n",
                        "Classes: [486 128 206 194  66]\n"
                    ]
                }
            ],
            "source": [
                "# Cell 2: Load Data\n",
                "DATA_DIR = Path('data')\n",
                "with open(DATA_DIR / 'train_data.pkl', 'rb') as f:\n",
                "    train_data = pickle.load(f)\n",
                "with open(DATA_DIR / 'test_data.pkl', 'rb') as f:\n",
                "    test_data = pickle.load(f)\n",
                "\n",
                "X_train = train_data['images']\n",
                "y_train = train_data['labels'].flatten().astype(np.int64)\n",
                "X_test = test_data['images']\n",
                "\n",
                "if X_train.max() <= 1.0:\n",
                "    X_train = (X_train * 255).astype(np.uint8)\n",
                "    X_test = (X_test * 255).astype(np.uint8)\n",
                "else:\n",
                "    X_train = X_train.astype(np.uint8)\n",
                "    X_test = X_test.astype(np.uint8)\n",
                "\n",
                "n_train, n_test = len(X_train), len(X_test)\n",
                "print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")\n",
                "print(f\"Classes: {np.bincount(y_train)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Extracting features...\n",
                        "Feature shape: (1080, 2390)\n"
                    ]
                }
            ],
            "source": [
                "# Cell 3: Enhanced Feature Engineering\n",
                "def compute_gradient_hist(img_gray, bins=8):\n",
                "    \"\"\"Compute simple gradient histogram\"\"\"\n",
                "    gx = np.diff(img_gray, axis=1)\n",
                "    gy = np.diff(img_gray, axis=0)\n",
                "    # Resize to same shape\n",
                "    gx = gx[:27, :]\n",
                "    gy = gy[:, :27]\n",
                "    mag = np.sqrt(gx**2 + gy**2)\n",
                "    hist, _ = np.histogram(mag.flatten(), bins=bins, range=(0, 255))\n",
                "    return hist.astype(np.float32) / (hist.sum() + 1e-6)\n",
                "\n",
                "def extract_features(images):\n",
                "    \"\"\"Extract features: flatten pixels + color statistics + gradient hist\"\"\"\n",
                "    features = []\n",
                "    for img in images:\n",
                "        # Flatten RGB pixels\n",
                "        flat = img.flatten().astype(np.float32) / 255.0\n",
                "        \n",
                "        # Color statistics per channel\n",
                "        stats = []\n",
                "        for c in range(3):\n",
                "            ch = img[:, :, c].astype(np.float32)\n",
                "            stats.extend([ch.mean(), ch.std(), ch.min(), ch.max(),\n",
                "                          np.percentile(ch, 25), np.percentile(ch, 75),\n",
                "                          np.percentile(ch, 10), np.percentile(ch, 90)])  # More percentiles\n",
                "        \n",
                "        # Grayscale stats\n",
                "        gray = 0.299*img[:,:,0] + 0.587*img[:,:,1] + 0.114*img[:,:,2]\n",
                "        stats.extend([gray.mean(), gray.std(), gray.min(), gray.max(),\n",
                "                      np.percentile(gray, 25), np.percentile(gray, 75)])\n",
                "        \n",
                "        # Gradient histogram\n",
                "        grad_hist = compute_gradient_hist(gray)\n",
                "        \n",
                "        # Combine\n",
                "        features.append(np.concatenate([flat, np.array(stats, dtype=np.float32), grad_hist]))\n",
                "    return np.array(features, dtype=np.float32)\n",
                "\n",
                "print(\"Extracting features...\")\n",
                "X_train_feat = extract_features(X_train)\n",
                "X_test_feat = extract_features(X_test)\n",
                "print(f\"Feature shape: {X_train_feat.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model defined.\n"
                    ]
                }
            ],
            "source": [
                "# Cell 4: Dataset & Wider CNN Model\n",
                "class ImgDataset(Dataset):\n",
                "    def __init__(self, images, labels=None, transform=None):\n",
                "        self.images = images\n",
                "        self.labels = labels\n",
                "        self.transform = transform\n",
                "        \n",
                "    def __len__(self): return len(self.images)\n",
                "    \n",
                "    def __getitem__(self, i):\n",
                "        img = Image.fromarray(self.images[i].astype(np.uint8))\n",
                "        if self.transform: img = self.transform(img)\n",
                "        if self.labels is not None:\n",
                "            return img, torch.tensor(self.labels[i], dtype=torch.long)\n",
                "        return img\n",
                "\n",
                "# Enhanced data augmentation with RandomErasing\n",
                "train_tf = transforms.Compose([\n",
                "    transforms.RandomHorizontalFlip(),\n",
                "    transforms.RandomRotation(20),\n",
                "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
                "    transforms.ColorJitter(0.15, 0.15, 0.15, 0.05),\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
                "    transforms.RandomErasing(p=0.3, scale=(0.02, 0.15))\n",
                "])\n",
                "\n",
                "val_tf = transforms.Compose([\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
                "])\n",
                "\n",
                "# TTA transforms (lighter augmentation for test time)\n",
                "tta_tf = transforms.Compose([\n",
                "    transforms.RandomHorizontalFlip(p=0.5),\n",
                "    transforms.RandomRotation(10),\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
                "])\n",
                "\n",
                "class ResBlock(nn.Module):\n",
                "    def __init__(self, ch):\n",
                "        super().__init__()\n",
                "        self.conv1 = nn.Conv2d(ch, ch, 3, padding=1, bias=False)\n",
                "        self.bn1 = nn.BatchNorm2d(ch)\n",
                "        self.conv2 = nn.Conv2d(ch, ch, 3, padding=1, bias=False)\n",
                "        self.bn2 = nn.BatchNorm2d(ch)\n",
                "    def forward(self, x):\n",
                "        return F.relu(self.bn2(self.conv2(F.relu(self.bn1(self.conv1(x))))) + x)\n",
                "\n",
                "# Wider CNN with more channels\n",
                "class CNN(nn.Module):\n",
                "    def __init__(self, nc=5):\n",
                "        super().__init__()\n",
                "        self.layers = nn.Sequential(\n",
                "            # Block 1: 28x28 -> 14x14\n",
                "            nn.Conv2d(3, 96, 3, padding=1), nn.BatchNorm2d(96), nn.ReLU(),\n",
                "            nn.MaxPool2d(2),\n",
                "            ResBlock(96),\n",
                "            \n",
                "            # Block 2: 14x14 -> 7x7\n",
                "            nn.Conv2d(96, 192, 3, padding=1), nn.BatchNorm2d(192), nn.ReLU(),\n",
                "            nn.MaxPool2d(2),\n",
                "            ResBlock(192),\n",
                "            \n",
                "            # Block 3: 7x7 -> global\n",
                "            nn.Conv2d(192, 384, 3, padding=1), nn.BatchNorm2d(384), nn.ReLU(),\n",
                "            nn.AdaptiveAvgPool2d(1),\n",
                "            nn.Flatten(),\n",
                "            nn.Dropout(0.5),\n",
                "            nn.Linear(384, 128),\n",
                "            nn.ReLU(),\n",
                "            nn.Dropout(0.3),\n",
                "            nn.Linear(128, nc)\n",
                "        )\n",
                "    def forward(self, x): return self.layers(x)\n",
                "\n",
                "print(\"Model defined.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Helpers defined.\n"
                    ]
                }
            ],
            "source": [
                "# Cell 5: Helpers with TTA\n",
                "def get_sampler(y):\n",
                "    w = 1.0 / np.maximum(np.bincount(y), 1)\n",
                "    return WeightedRandomSampler(torch.from_numpy(w[y]).double(), len(y))\n",
                "\n",
                "def train_ep(model, loader, crit, opt, dev):\n",
                "    model.train()\n",
                "    for x, y in loader:\n",
                "        x, y = x.to(dev), y.to(dev)\n",
                "        opt.zero_grad()\n",
                "        loss = crit(model(x), y)\n",
                "        loss.backward()\n",
                "        opt.step()\n",
                "\n",
                "@torch.no_grad()\n",
                "def val_ep(model, loader, dev):\n",
                "    model.eval()\n",
                "    c, t, probs = 0, 0, []\n",
                "    for x, y in loader:\n",
                "        x, y = x.to(dev), y.to(dev)\n",
                "        out = model(x)\n",
                "        c += (out.argmax(1) == y).sum().item()\n",
                "        t += y.size(0)\n",
                "        probs.append(F.softmax(out, 1).cpu().numpy())\n",
                "    return c / t, np.concatenate(probs)\n",
                "\n",
                "@torch.no_grad()\n",
                "def predict(model, loader, dev):\n",
                "    model.eval()\n",
                "    probs = []\n",
                "    for x in loader:\n",
                "        probs.append(F.softmax(model(x.to(dev)), 1).cpu().numpy())\n",
                "    return np.concatenate(probs)\n",
                "\n",
                "@torch.no_grad()\n",
                "def predict_tta(model, images, dev, n_tta=5):\n",
                "    \"\"\"Test-Time Augmentation: multiple forward passes with augmentation\"\"\"\n",
                "    model.eval()\n",
                "    all_probs = []\n",
                "    \n",
                "    # Original prediction\n",
                "    ds = ImgDataset(images, None, val_tf)\n",
                "    loader = DataLoader(ds, 64, shuffle=False, num_workers=0)\n",
                "    probs = predict(model, loader, dev)\n",
                "    all_probs.append(probs)\n",
                "    \n",
                "    # TTA predictions\n",
                "    for _ in range(n_tta - 1):\n",
                "        ds = ImgDataset(images, None, tta_tf)\n",
                "        loader = DataLoader(ds, 64, shuffle=False, num_workers=0)\n",
                "        probs = predict(model, loader, dev)\n",
                "        all_probs.append(probs)\n",
                "    \n",
                "    return np.mean(all_probs, axis=0)\n",
                "\n",
                "print(\"Helpers defined.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Training: 5 folds, 35 epochs, TTA=4\n",
                        "==================================================\n",
                        "\n",
                        "Fold 1/5\n",
                        "  CNN: 0.4769\n",
                        "  ET:  0.5046\n",
                        "  RF:  0.5231\n",
                        "  HGB: 0.4954\n",
                        "  Time: 4.9min\n",
                        "\n",
                        "Fold 2/5\n",
                        "  CNN: 0.4630\n",
                        "  ET:  0.5000\n",
                        "  RF:  0.5000\n",
                        "  HGB: 0.5046\n",
                        "  Time: 6.3min\n",
                        "\n",
                        "Fold 3/5\n",
                        "  CNN: 0.4907\n",
                        "  ET:  0.5185\n",
                        "  RF:  0.5509\n",
                        "  HGB: 0.4861\n",
                        "  Time: 6.8min\n",
                        "\n",
                        "Fold 4/5\n",
                        "  CNN: 0.4583\n",
                        "  ET:  0.5093\n",
                        "  RF:  0.5000\n",
                        "  HGB: 0.4954\n",
                        "  Time: 6.7min\n",
                        "\n",
                        "Fold 5/5\n",
                        "  CNN: 0.4306\n",
                        "  ET:  0.4676\n",
                        "  RF:  0.5000\n",
                        "  HGB: 0.4583\n",
                        "  Time: 7.0min\n",
                        "\n",
                        "Total: 31.8min\n"
                    ]
                }
            ],
            "source": [
                "# Cell 6: Training\n",
                "N_FOLDS, EPOCHS, BS = 5, 35, 64\n",
                "N_TTA = 4  # Number of TTA passes\n",
                "skf = StratifiedKFold(N_FOLDS, shuffle=True, random_state=SEED)\n",
                "\n",
                "# Storage\n",
                "cnn_oof = np.zeros((n_train, 5))\n",
                "cnn_test = np.zeros((n_test, 5))\n",
                "et_oof = np.zeros((n_train, 5))\n",
                "et_test = np.zeros((n_test, 5))\n",
                "rf_oof = np.zeros((n_train, 5))\n",
                "rf_test = np.zeros((n_test, 5))\n",
                "hgb_oof = np.zeros((n_train, 5))\n",
                "hgb_test = np.zeros((n_test, 5))\n",
                "\n",
                "print(f\"Training: {N_FOLDS} folds, {EPOCHS} epochs, TTA={N_TTA}\")\n",
                "print(\"=\"*50)\n",
                "\n",
                "for fold, (tr_i, val_i) in enumerate(skf.split(X_train, y_train)):\n",
                "    t1 = time.time()\n",
                "    print(f\"\\nFold {fold+1}/{N_FOLDS}\")\n",
                "    \n",
                "    # CNN\n",
                "    tr_ds = ImgDataset(X_train[tr_i], y_train[tr_i], train_tf)\n",
                "    val_ds = ImgDataset(X_train[val_i], y_train[val_i], val_tf)\n",
                "    tr_ld = DataLoader(tr_ds, BS, sampler=get_sampler(y_train[tr_i]), num_workers=0)\n",
                "    val_ld = DataLoader(val_ds, BS, shuffle=False, num_workers=0)\n",
                "    \n",
                "    model = CNN().to(device)\n",
                "    crit = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
                "    opt = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
                "    sch = optim.lr_scheduler.CosineAnnealingWarmRestarts(opt, T_0=10, T_mult=2)\n",
                "    \n",
                "    best_acc, best_w = 0, None\n",
                "    for ep in range(EPOCHS):\n",
                "        train_ep(model, tr_ld, crit, opt, device)\n",
                "        acc, _ = val_ep(model, val_ld, device)\n",
                "        sch.step()\n",
                "        if acc > best_acc:\n",
                "            best_acc = acc\n",
                "            best_w = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
                "    \n",
                "    model.load_state_dict(best_w)\n",
                "    _, probs = val_ep(model, val_ld, device)\n",
                "    cnn_oof[val_i] = probs\n",
                "    \n",
                "    # TTA for test predictions\n",
                "    cnn_test += predict_tta(model, X_test, device, N_TTA) / N_FOLDS\n",
                "    print(f\"  CNN: {best_acc:.4f}\")\n",
                "    \n",
                "    # ET with more trees\n",
                "    et = ExtraTreesClassifier(300, max_depth=30, min_samples_split=3, \n",
                "                              class_weight='balanced', random_state=SEED+fold, n_jobs=-1)\n",
                "    et.fit(X_train_feat[tr_i], y_train[tr_i])\n",
                "    et_oof[val_i] = et.predict_proba(X_train_feat[val_i])\n",
                "    et_test += et.predict_proba(X_test_feat) / N_FOLDS\n",
                "    print(f\"  ET:  {et.score(X_train_feat[val_i], y_train[val_i]):.4f}\")\n",
                "    \n",
                "    # RF with more trees\n",
                "    rf = RandomForestClassifier(300, max_depth=30, min_samples_split=3,\n",
                "                                class_weight='balanced', random_state=SEED+fold, n_jobs=-1)\n",
                "    rf.fit(X_train_feat[tr_i], y_train[tr_i])\n",
                "    rf_oof[val_i] = rf.predict_proba(X_train_feat[val_i])\n",
                "    rf_test += rf.predict_proba(X_test_feat) / N_FOLDS\n",
                "    print(f\"  RF:  {rf.score(X_train_feat[val_i], y_train[val_i]):.4f}\")\n",
                "    \n",
                "    # HistGradientBoosting with more iterations  \n",
                "    hgb = HistGradientBoostingClassifier(max_iter=150, max_depth=12, \n",
                "                                        learning_rate=0.05, random_state=SEED+fold)\n",
                "    hgb.fit(X_train_feat[tr_i], y_train[tr_i])\n",
                "    hgb_oof[val_i] = hgb.predict_proba(X_train_feat[val_i])\n",
                "    hgb_test += hgb.predict_proba(X_test_feat) / N_FOLDS\n",
                "    print(f\"  HGB: {hgb.score(X_train_feat[val_i], y_train[val_i]):.4f}\")\n",
                "    \n",
                "    print(f\"  Time: {(time.time()-t1)/60:.1f}min\")\n",
                "\n",
                "print(f\"\\nTotal: {(time.time()-t0)/60:.1f}min\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Individual CV Accuracy:\n",
                        "  CNN: 0.4639\n",
                        "  ET:  0.5000\n",
                        "  RF:  0.5148\n",
                        "  HGB: 0.4880\n",
                        "\n",
                        "Best weights: CNN=0.25, ET=0.35, RF=0.30, HGB=0.10\n",
                        "Ensemble CV: 0.5111\n"
                    ]
                }
            ],
            "source": [
                "# Cell 7: Fine-grained Ensemble Weight Search\n",
                "print(\"Individual CV Accuracy:\")\n",
                "print(f\"  CNN: {accuracy_score(y_train, cnn_oof.argmax(1)):.4f}\")\n",
                "print(f\"  ET:  {accuracy_score(y_train, et_oof.argmax(1)):.4f}\")\n",
                "print(f\"  RF:  {accuracy_score(y_train, rf_oof.argmax(1)):.4f}\")\n",
                "print(f\"  HGB: {accuracy_score(y_train, hgb_oof.argmax(1)):.4f}\")\n",
                "\n",
                "# Fine-grained grid search (0.05 step)\n",
                "best_acc, best_w = 0, None\n",
                "for w1 in np.arange(0.2, 0.65, 0.05):  # CNN\n",
                "    for w2 in np.arange(0.1, 0.45, 0.05):  # ET\n",
                "        for w3 in np.arange(0.05, 0.35, 0.05):  # RF\n",
                "            w4 = 1 - w1 - w2 - w3\n",
                "            if w4 < 0 or w4 > 0.4: continue\n",
                "            oof = w1*cnn_oof + w2*et_oof + w3*rf_oof + w4*hgb_oof\n",
                "            acc = accuracy_score(y_train, oof.argmax(1))\n",
                "            if acc > best_acc:\n",
                "                best_acc, best_w = acc, (w1, w2, w3, w4)\n",
                "\n",
                "print(f\"\\nBest weights: CNN={best_w[0]:.2f}, ET={best_w[1]:.2f}, RF={best_w[2]:.2f}, HGB={best_w[3]:.2f}\")\n",
                "print(f\"Ensemble CV: {best_acc:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Saved: submission_milestone2_v5.csv\n",
                        "\n",
                        "Distribution:\n",
                        "Label\n",
                        "0    252\n",
                        "1     35\n",
                        "2     51\n",
                        "3     61\n",
                        "4      1\n",
                        "Name: count, dtype: int64\n",
                        "\n",
                        "Total time: 31.8min\n"
                    ]
                }
            ],
            "source": [
                "# Cell 8: Submission\n",
                "w1, w2, w3, w4 = best_w\n",
                "final = w1*cnn_test + w2*et_test + w3*rf_test + w4*hgb_test\n",
                "preds = final.argmax(1)\n",
                "\n",
                "# IMPORTANT: ID starts from 1!\n",
                "submission = pd.DataFrame({'ID': np.arange(1, n_test + 1), 'Label': preds})\n",
                "submission.to_csv('submission_milestone2_v5.csv', index=False)\n",
                "\n",
                "print(\"Saved: submission_milestone2_v5.csv\")\n",
                "print(f\"\\nDistribution:\\n{submission['Label'].value_counts().sort_index()}\")\n",
                "print(f\"\\nTotal time: {(time.time()-t0)/60:.1f}min\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.14.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
