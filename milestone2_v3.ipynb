{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# IFT3395 Competition 2 - Milestone 2 v3 (Fast Version)\n",
                "\n",
                "## 目标: Validation Accuracy > 0.53, 训练时间 < 15分钟\n",
                "\n",
                "## 优化策略\n",
                "- **3折CV** (而非5折) - 减少40%训练时间\n",
                "- **20 Epochs** - 足够收敛\n",
                "- **更小的Sklearn模型** - 150棵树\n",
                "- **简化CNN** - 更少的通道数"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Device: cpu\n"
                    ]
                }
            ],
            "source": [
                "# ==================== Cell 1: Imports ====================\n",
                "import pickle\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from pathlib import Path\n",
                "import random\n",
                "import os\n",
                "import warnings\n",
                "import time\n",
                "from PIL import Image\n",
                "\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
                "from torchvision import transforms\n",
                "\n",
                "from sklearn.model_selection import StratifiedKFold\n",
                "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
                "from sklearn.svm import SVC\n",
                "from sklearn.metrics import accuracy_score\n",
                "\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "SEED = 42\n",
                "def seed_everything(seed):\n",
                "    random.seed(seed)\n",
                "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
                "    np.random.seed(seed)\n",
                "    torch.manual_seed(seed)\n",
                "    if torch.cuda.is_available():\n",
                "        torch.cuda.manual_seed(seed)\n",
                "        torch.backends.cudnn.deterministic = True\n",
                "\n",
                "seed_everything(SEED)\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Device: {device}\")\n",
                "start_time = time.time()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Train: (1080, 28, 28, 3), Test: (400, 28, 28, 3)\n",
                        "Classes: [486 128 206 194  66]\n"
                    ]
                }
            ],
            "source": [
                "# ==================== Cell 2: Load Data ====================\n",
                "DATA_DIR = Path('data')\n",
                "\n",
                "with open(DATA_DIR / 'train_data.pkl', 'rb') as f:\n",
                "    train_data = pickle.load(f)\n",
                "with open(DATA_DIR / 'test_data.pkl', 'rb') as f:\n",
                "    test_data = pickle.load(f)\n",
                "\n",
                "X_train_raw = train_data['images']\n",
                "y_train = train_data['labels'].flatten().astype(np.int64)\n",
                "X_test_raw = test_data['images']\n",
                "\n",
                "if X_train_raw.max() <= 1.0:\n",
                "    X_train_raw = (X_train_raw * 255).astype(np.uint8)\n",
                "    X_test_raw = (X_test_raw * 255).astype(np.uint8)\n",
                "else:\n",
                "    X_train_raw = X_train_raw.astype(np.uint8)\n",
                "    X_test_raw = X_test_raw.astype(np.uint8)\n",
                "\n",
                "print(f\"Train: {X_train_raw.shape}, Test: {X_test_raw.shape}\")\n",
                "print(f\"Classes: {np.bincount(y_train)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model and dataset defined.\n"
                    ]
                }
            ],
            "source": [
                "# ==================== Cell 3: Dataset & Model ====================\n",
                "class SimpleDataset(Dataset):\n",
                "    def __init__(self, images, labels=None, transform=None):\n",
                "        self.images = images\n",
                "        self.labels = labels\n",
                "        self.transform = transform\n",
                "        \n",
                "    def __len__(self):\n",
                "        return len(self.images)\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        img = Image.fromarray(self.images[idx].astype(np.uint8))\n",
                "        if self.transform:\n",
                "            img = self.transform(img)\n",
                "        if self.labels is not None:\n",
                "            return img, torch.tensor(self.labels[idx], dtype=torch.long)\n",
                "        return img\n",
                "\n",
                "# Transforms\n",
                "train_tf = transforms.Compose([\n",
                "    transforms.RandomHorizontalFlip(),\n",
                "    transforms.RandomRotation(10),\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
                "])\n",
                "\n",
                "val_tf = transforms.Compose([\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
                "])\n",
                "\n",
                "# Fast CNN - fewer channels\n",
                "class FastCNN(nn.Module):\n",
                "    def __init__(self, num_classes=5):\n",
                "        super().__init__()\n",
                "        self.features = nn.Sequential(\n",
                "            nn.Conv2d(3, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(),\n",
                "            nn.Conv2d(32, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(),\n",
                "            nn.MaxPool2d(2),\n",
                "            \n",
                "            nn.Conv2d(32, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(),\n",
                "            nn.Conv2d(64, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(),\n",
                "            nn.MaxPool2d(2),\n",
                "            \n",
                "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(),\n",
                "            nn.Conv2d(128, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(),\n",
                "            nn.AdaptiveAvgPool2d(1)\n",
                "        )\n",
                "        self.classifier = nn.Sequential(\n",
                "            nn.Flatten(),\n",
                "            nn.Dropout(0.3),\n",
                "            nn.Linear(128, num_classes)\n",
                "        )\n",
                "        \n",
                "    def forward(self, x):\n",
                "        return self.classifier(self.features(x))\n",
                "\n",
                "print(\"Model and dataset defined.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Helper functions defined.\n"
                    ]
                }
            ],
            "source": [
                "# ==================== Cell 4: Helper Functions ====================\n",
                "def get_sampler(labels):\n",
                "    counts = np.bincount(labels)\n",
                "    weights = 1.0 / np.maximum(counts, 1)\n",
                "    sample_weights = weights[labels]\n",
                "    return WeightedRandomSampler(torch.from_numpy(sample_weights).double(), len(sample_weights))\n",
                "\n",
                "def train_epoch(model, loader, criterion, optimizer, device):\n",
                "    model.train()\n",
                "    correct, total = 0, 0\n",
                "    for imgs, labels in loader:\n",
                "        imgs, labels = imgs.to(device), labels.to(device)\n",
                "        optimizer.zero_grad()\n",
                "        out = model(imgs)\n",
                "        loss = criterion(out, labels)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        _, pred = out.max(1)\n",
                "        total += labels.size(0)\n",
                "        correct += pred.eq(labels).sum().item()\n",
                "    return correct / total\n",
                "\n",
                "@torch.no_grad()\n",
                "def validate(model, loader, device):\n",
                "    model.eval()\n",
                "    correct, total = 0, 0\n",
                "    probs = []\n",
                "    for imgs, labels in loader:\n",
                "        imgs, labels = imgs.to(device), labels.to(device)\n",
                "        out = model(imgs)\n",
                "        _, pred = out.max(1)\n",
                "        total += labels.size(0)\n",
                "        correct += pred.eq(labels).sum().item()\n",
                "        probs.append(F.softmax(out, dim=1).cpu().numpy())\n",
                "    return correct / total, np.concatenate(probs)\n",
                "\n",
                "@torch.no_grad()\n",
                "def predict(model, loader, device):\n",
                "    model.eval()\n",
                "    probs = []\n",
                "    for imgs in loader:\n",
                "        imgs = imgs.to(device)\n",
                "        out = model(imgs)\n",
                "        probs.append(F.softmax(out, dim=1).cpu().numpy())\n",
                "    return np.concatenate(probs)\n",
                "\n",
                "print(\"Helper functions defined.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Config: 3 folds, 20 epochs, batch=64\n",
                        "==================================================\n"
                    ]
                }
            ],
            "source": [
                "# ==================== Cell 5: Fast Training ====================\n",
                "N_FOLDS = 3  # 3折更快\n",
                "EPOCHS = 20  # 20个epoch足够\n",
                "BATCH_SIZE = 64  # 大batch更快\n",
                "\n",
                "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
                "\n",
                "# Preallocate\n",
                "cnn_oof = np.zeros((len(y_train), 5))\n",
                "cnn_test = np.zeros((len(X_test_raw), 5))\n",
                "et_oof = np.zeros((len(y_train), 5))\n",
                "et_test = np.zeros((len(X_test_raw), 5))\n",
                "rf_oof = np.zeros((len(y_train), 5))\n",
                "rf_test = np.zeros((len(X_test_raw), 5))\n",
                "\n",
                "# Flatten for sklearn\n",
                "X_flat = X_train_raw.reshape(len(X_train_raw), -1).astype(np.float32) / 255.0\n",
                "X_test_flat = X_test_raw.reshape(len(X_test_raw), -1).astype(np.float32) / 255.0\n",
                "\n",
                "print(f\"Config: {N_FOLDS} folds, {EPOCHS} epochs, batch={BATCH_SIZE}\")\n",
                "print(\"=\"*50)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "--- Fold 1/3 ---\n",
                        "  CNN: 0.4083\n",
                        "  ET:  0.4917\n",
                        "  RF:  0.4833\n",
                        "  Fold time: 0.5 min\n",
                        "\n",
                        "--- Fold 2/3 ---\n",
                        "  CNN: 0.4861\n",
                        "  ET:  0.5000\n",
                        "  RF:  0.5389\n",
                        "  Fold time: 0.5 min\n",
                        "\n",
                        "--- Fold 3/3 ---\n",
                        "  CNN: 0.4194\n",
                        "  ET:  0.4917\n",
                        "  RF:  0.4861\n",
                        "  Fold time: 0.5 min\n",
                        "\n",
                        "Total training time: 1.5 min\n"
                    ]
                }
            ],
            "source": [
                "# ==================== Cell 6: Main Training Loop ====================\n",
                "for fold, (tr_idx, val_idx) in enumerate(skf.split(X_train_raw, y_train)):\n",
                "    fold_start = time.time()\n",
                "    print(f\"\\n--- Fold {fold+1}/{N_FOLDS} ---\")\n",
                "    \n",
                "    X_tr, y_tr = X_train_raw[tr_idx], y_train[tr_idx]\n",
                "    X_val, y_val = X_train_raw[val_idx], y_train[val_idx]\n",
                "    \n",
                "    # === CNN ===\n",
                "    train_ds = SimpleDataset(X_tr, y_tr, train_tf)\n",
                "    val_ds = SimpleDataset(X_val, y_val, val_tf)\n",
                "    train_loader = DataLoader(train_ds, BATCH_SIZE, sampler=get_sampler(y_tr), num_workers=0)\n",
                "    val_loader = DataLoader(val_ds, BATCH_SIZE, shuffle=False, num_workers=0)\n",
                "    \n",
                "    model = FastCNN(5).to(device)\n",
                "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
                "    optimizer = optim.AdamW(model.parameters(), lr=0.002, weight_decay=1e-4)\n",
                "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
                "    \n",
                "    best_acc, best_weights = 0, None\n",
                "    for ep in range(EPOCHS):\n",
                "        train_epoch(model, train_loader, criterion, optimizer, device)\n",
                "        val_acc, _ = validate(model, val_loader, device)\n",
                "        scheduler.step()\n",
                "        if val_acc > best_acc:\n",
                "            best_acc = val_acc\n",
                "            best_weights = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
                "    \n",
                "    model.load_state_dict(best_weights)\n",
                "    _, val_probs = validate(model, val_loader, device)\n",
                "    cnn_oof[val_idx] = val_probs\n",
                "    \n",
                "    # Test prediction\n",
                "    test_ds = SimpleDataset(X_test_raw, None, val_tf)\n",
                "    test_loader = DataLoader(test_ds, BATCH_SIZE, shuffle=False, num_workers=0)\n",
                "    cnn_test += predict(model, test_loader, device) / N_FOLDS\n",
                "    print(f\"  CNN: {best_acc:.4f}\")\n",
                "    \n",
                "    # === ExtraTrees (快速版) ===\n",
                "    et = ExtraTreesClassifier(n_estimators=150, max_depth=20, class_weight='balanced', \n",
                "                              random_state=SEED+fold, n_jobs=-1)\n",
                "    et.fit(X_flat[tr_idx], y_train[tr_idx])\n",
                "    et_acc = et.score(X_flat[val_idx], y_train[val_idx])\n",
                "    et_oof[val_idx] = et.predict_proba(X_flat[val_idx])\n",
                "    et_test += et.predict_proba(X_test_flat) / N_FOLDS\n",
                "    print(f\"  ET:  {et_acc:.4f}\")\n",
                "    \n",
                "    # === RandomForest (快速版) ===\n",
                "    rf = RandomForestClassifier(n_estimators=150, max_depth=20, class_weight='balanced',\n",
                "                                random_state=SEED+fold, n_jobs=-1)\n",
                "    rf.fit(X_flat[tr_idx], y_train[tr_idx])\n",
                "    rf_acc = rf.score(X_flat[val_idx], y_train[val_idx])\n",
                "    rf_oof[val_idx] = rf.predict_proba(X_flat[val_idx])\n",
                "    rf_test += rf.predict_proba(X_test_flat) / N_FOLDS\n",
                "    print(f\"  RF:  {rf_acc:.4f}\")\n",
                "    \n",
                "    print(f\"  Fold time: {(time.time()-fold_start)/60:.1f} min\")\n",
                "\n",
                "print(f\"\\nTotal training time: {(time.time()-start_time)/60:.1f} min\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Individual Model CV Accuracy:\n",
                        "  CNN: 0.4380\n",
                        "  ET:  0.4944\n",
                        "  RF:  0.5028\n",
                        "\n",
                        "Best Ensemble: CNN=0.5, ET=0.2, RF=0.3\n",
                        "Ensemble CV Accuracy: 0.4972\n"
                    ]
                }
            ],
            "source": [
                "# ==================== Cell 7: Ensemble ====================\n",
                "cnn_acc = accuracy_score(y_train, np.argmax(cnn_oof, axis=1))\n",
                "et_acc = accuracy_score(y_train, np.argmax(et_oof, axis=1))\n",
                "rf_acc = accuracy_score(y_train, np.argmax(rf_oof, axis=1))\n",
                "\n",
                "print(\"Individual Model CV Accuracy:\")\n",
                "print(f\"  CNN: {cnn_acc:.4f}\")\n",
                "print(f\"  ET:  {et_acc:.4f}\")\n",
                "print(f\"  RF:  {rf_acc:.4f}\")\n",
                "\n",
                "# 快速权重搜索\n",
                "best_acc, best_w = 0, None\n",
                "for w1 in [0.3, 0.4, 0.5, 0.6]:\n",
                "    for w2 in [0.2, 0.3, 0.4]:\n",
                "        w3 = 1 - w1 - w2\n",
                "        if w3 < 0: continue\n",
                "        oof = w1 * cnn_oof + w2 * et_oof + w3 * rf_oof\n",
                "        acc = accuracy_score(y_train, np.argmax(oof, axis=1))\n",
                "        if acc > best_acc:\n",
                "            best_acc, best_w = acc, (w1, w2, w3)\n",
                "\n",
                "print(f\"\\nBest Ensemble: CNN={best_w[0]}, ET={best_w[1]}, RF={best_w[2]}\")\n",
                "print(f\"Ensemble CV Accuracy: {best_acc:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Saved: submission_milestone2_v3.csv\n",
                        "\n",
                        "Prediction distribution:\n",
                        "Label\n",
                        "0    208\n",
                        "1     60\n",
                        "2     35\n",
                        "3     89\n",
                        "4      8\n",
                        "Name: count, dtype: int64\n",
                        "\n",
                        "Total time: 1.5 min\n"
                    ]
                }
            ],
            "source": [
                "# ==================== Cell 8: Submission ====================\n",
                "w1, w2, w3 = best_w\n",
                "final_probs = w1 * cnn_test + w2 * et_test + w3 * rf_test\n",
                "preds = np.argmax(final_probs, axis=1)\n",
                "\n",
                "submission = pd.DataFrame({'ImageId': np.arange(len(preds)), 'Label': preds})\n",
                "submission.to_csv('submission_milestone2_v3.csv', index=False)\n",
                "\n",
                "print(\"Saved: submission_milestone2_v3.csv\")\n",
                "print(f\"\\nPrediction distribution:\")\n",
                "print(submission['Label'].value_counts().sort_index())\n",
                "print(f\"\\nTotal time: {(time.time()-start_time)/60:.1f} min\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.14.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
