{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# IFT3395 Competition 2 - Milestone 2: Ultimate Ensemble\n",
                "\n",
                "## 目标 Goal\n",
                "提高Kaggle分数至 > 0.53。\n",
                "\n",
                "## 策略 Strategy (核心改动)\n",
                "1.  **完全不使用预训练模型 (No Pre-trained Models)**：\n",
                "    *   我们设计了一个自定义的轻量级CNN (**Custom ResNet9 Variant**)，完全**从这开始训练 (Trained from Scratch)**。\n",
                "    *   这符合比赛规则，且针对28x28的小图像进行了优化（避免了ResNet50等大模型在小图上的过拟合问题）。\n",
                "\n",
                "2.  **解决类别不平衡 (Handle Class Imbalance)**：\n",
                "    *   数据极度不平衡 (Class 0: 486 vs Class 4: 66)。\n",
                "    *   **PyTorch**: 使用 `WeightedRandomSampler`，确保每个Batch中各类别的样本数量大致相等。\n",
                "    *   **Sklearn**: 使用 `class_weight='balanced'`。\n",
                "\n",
                "3.  **强力集成 (Robust Ensemble)**：\n",
                "    *   **CNN (Deep Learning)**: 捕捉空间特征。\n",
                "    *   **ExtraTrees (Machine Learning)**: 捕捉统计特征，在这个数据集上表现非常稳健。\n",
                "    *   结果 = 0.6 * CNN预测 + 0.4 * ExtraTrees预测。\n",
                "\n",
                "4.  **测试时增强 (TTA)**：\n",
                "    *   预测时对测试图片进行水平翻转、旋转，取平均值，提高泛化能力。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "b8468fbe",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Using device: cpu\n"
                    ]
                }
            ],
            "source": [
                "import pickle\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from pathlib import Path\n",
                "import random\n",
                "import os\n",
                "import warnings\n",
                "from PIL import Image\n",
                "\n",
                "# PyTorch\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
                "from torchvision import transforms\n",
                "\n",
                "# Sklearn\n",
                "from sklearn.model_selection import StratifiedKFold\n",
                "from sklearn.ensemble import ExtraTreesClassifier\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.metrics import accuracy_score\n",
                "\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# 设定随机种子\n",
                "SEED = 42\n",
                "def seed_everything(seed):\n",
                "    random.seed(seed)\n",
                "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
                "    np.random.seed(seed)\n",
                "    torch.manual_seed(seed)\n",
                "    if torch.cuda.is_available():\n",
                "        torch.cuda.manual_seed(seed)\n",
                "        torch.backends.cudnn.deterministic = True\n",
                "        torch.backends.cudnn.benchmark = False\n",
                "\n",
                "seed_everything(SEED)\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1db99a17",
            "metadata": {},
            "source": [
                "## 1. 数据加载与预处理 Data Loading"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "04c63d6c",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Train Shape: (1080, 28, 28, 3)\n",
                        "Class Counts: [486 128 206 194  66]\n"
                    ]
                }
            ],
            "source": [
                "DATA_DIR = Path('data')\n",
                "\n",
                "def load_data():\n",
                "    with open(DATA_DIR / 'train_data.pkl', 'rb') as f:\n",
                "        train = pickle.load(f)\n",
                "    with open(DATA_DIR / 'test_data.pkl', 'rb') as f:\n",
                "        test = pickle.load(f)\n",
                "    return train, test\n",
                "\n",
                "train_data, test_data = load_data()\n",
                "\n",
                "X_train_raw = train_data['images'] # (1080, 28, 28, 3)\n",
                "y_train = train_data['labels'].flatten() # (1080,)\n",
                "X_test_raw = test_data['images'] # (400, 28, 28, 3)\n",
                "\n",
                "# 确保是 float32 且在 0-255 之间，方便后续 transforms 处理\n",
                "if X_train_raw.max() <= 1.0:\n",
                "    X_train_raw = (X_train_raw * 255).astype(np.uint8)\n",
                "    X_test_raw = (X_test_raw * 255).astype(np.uint8)\n",
                "else:\n",
                "    X_train_raw = X_train_raw.astype(np.uint8)\n",
                "    X_test_raw = X_test_raw.astype(np.uint8)\n",
                "\n",
                "# FIX: Ensure labels are int64 for CrossEntropyLoss\n",
                "y_train = y_train.astype(np.int64)\n",
                "\n",
                "print(\"Train Shape:\", X_train_raw.shape)\n",
                "print(\"Class Counts:\", np.bincount(y_train))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b23b2631",
            "metadata": {},
            "source": [
                "## 2. 自定义PyTorch数据集与增强 Dataset & Augmentation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "e2dc2f62",
            "metadata": {},
            "outputs": [],
            "source": [
                "class IFT3395Dataset(Dataset):\n",
                "    def __init__(self, images, labels=None, transform=None):\n",
                "        self.images = images\n",
                "        self.labels = labels\n",
                "        self.transform = transform\n",
                "        \n",
                "    def __len__(self):\n",
                "        return len(self.images)\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        # Image data assumed to be uint8 (0-255)\n",
                "        img_arr = self.images[idx].astype(np.uint8) # Safety cast\n",
                "        img = Image.fromarray(img_arr)\n",
                "        \n",
                "        if self.transform:\n",
                "            img = self.transform(img)\n",
                "            \n",
                "        if self.labels is not None:\n",
                "            # Verify labels type inside dataset just in case\n",
                "            return img, torch.tensor(self.labels[idx], dtype=torch.long)\n",
                "        return img\n",
                "\n",
                "# 增强策略：针对小图，适当增强但不要扭曲太严重\n",
                "train_transform = transforms.Compose([\n",
                "    transforms.RandomHorizontalFlip(),\n",
                "    transforms.RandomRotation(15),\n",
                "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
                "    transforms.ToTensor(), # 0-1 float\n",
                "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
                "])\n",
                "\n",
                "val_transform = transforms.Compose([\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
                "])"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "9aea4692",
            "metadata": {},
            "source": [
                "## 3. 自定义轻量级模型 Custom ResNet9 (From Scratch)\n",
                "这是一个经典的“DavidNet”风格的9层残差网络，非常适合CIFAR-10等小图数据集，训练快且效果好。\n",
                "**注意：这里没有加载任何预训练权重。**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "422c86a9",
            "metadata": {},
            "outputs": [],
            "source": [
                "def conv_bn(in_channels, out_channels, pool=False):\n",
                "    layers = [\n",
                "        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
                "        nn.BatchNorm2d(out_channels),\n",
                "        nn.ReLU(inplace=True)\n",
                "    ]\n",
                "    if pool:\n",
                "        layers.append(nn.MaxPool2d(2))\n",
                "    return nn.Sequential(*layers)\n",
                "\n",
                "class CustomResNet9(nn.Module):\n",
                "    def __init__(self, num_classes=5):\n",
                "        super().__init__()\n",
                "        \n",
                "        # Prep: 3 -> 64\n",
                "        self.prep = conv_bn(3, 64)\n",
                "        \n",
                "        # Layer 1: 64 -> 128, MaxPool, ResBlock\n",
                "        self.layer1_conv = conv_bn(64, 128, pool=True)\n",
                "        self.layer1_res = nn.Sequential(\n",
                "            conv_bn(128, 128),\n",
                "            conv_bn(128, 128)\n",
                "        )\n",
                "        \n",
                "        # Layer 2: 128 -> 256, MaxPool\n",
                "        self.layer2_conv = conv_bn(128, 256, pool=True)\n",
                "        \n",
                "        # Layer 3: 256 -> 512, MaxPool, ResBlock\n",
                "        self.layer3_conv = conv_bn(256, 512, pool=True)\n",
                "        self.layer3_res = nn.Sequential(\n",
                "            conv_bn(512, 512),\n",
                "            conv_bn(512, 512)\n",
                "        )\n",
                "        \n",
                "        # Classifier\n",
                "        self.classifier = nn.Sequential(\n",
                "            nn.AdaptiveMaxPool2d(1),\n",
                "            nn.Flatten(),\n",
                "            nn.Dropout(0.2),\n",
                "            nn.Linear(512, num_classes)\n",
                "        )\n",
                "        \n",
                "    def forward(self, x):\n",
                "        out = self.prep(x)\n",
                "        \n",
                "        out = self.layer1_conv(out)\n",
                "        out = out + self.layer1_res(out) # Residual add\n",
                "        \n",
                "        out = self.layer2_conv(out)\n",
                "        \n",
                "        out = self.layer3_conv(out)\n",
                "        out = out + self.layer3_res(out) # Residual add\n",
                "        \n",
                "        out = self.classifier(out)\n",
                "        return out"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "452840cf",
            "metadata": {},
            "source": [
                "## 4. 训练辅助函数 Helper Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "8ce3de05",
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_sampler(labels):\n",
                "    # 计算类别权重，用于WeightedRandomSampler\n",
                "    class_counts = np.bincount(labels)\n",
                "    # Handle potential zero counts if subset misses a class (unlikely with StratifiedCV)\n",
                "    class_counts = np.maximum(class_counts, 1)\n",
                "    \n",
                "    class_weights = 1. / class_counts\n",
                "    sample_weights = class_weights[labels]\n",
                "    \n",
                "    # FIX: Ensure weights are double tensor for Sampler\n",
                "    return WeightedRandomSampler(torch.from_numpy(sample_weights).double(), len(sample_weights))\n",
                "\n",
                "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
                "    model.train()\n",
                "    running_loss = 0.0\n",
                "    correct = 0\n",
                "    total = 0\n",
                "    for imgs, labels in loader:\n",
                "        imgs, labels = imgs.to(device), labels.to(device)\n",
                "        \n",
                "        optimizer.zero_grad()\n",
                "        outputs = model(imgs)\n",
                "        loss = criterion(outputs, labels)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        \n",
                "        running_loss += loss.item() * imgs.size(0)\n",
                "        _, predicted = outputs.max(1)\n",
                "        total += labels.size(0)\n",
                "        correct += predicted.eq(labels).sum().item()\n",
                "    return running_loss / total, correct / total\n",
                "\n",
                "@torch.no_grad()\n",
                "def validate(model, loader, criterion, device):\n",
                "    model.eval()\n",
                "    running_loss = 0.0\n",
                "    correct = 0\n",
                "    total = 0\n",
                "    probs = []\n",
                "    \n",
                "    for imgs, labels in loader:\n",
                "        imgs, labels = imgs.to(device), labels.to(device)\n",
                "        outputs = model(imgs)\n",
                "        loss = criterion(outputs, labels)\n",
                "        \n",
                "        running_loss += loss.item() * imgs.size(0)\n",
                "        _, predicted = outputs.max(1)\n",
                "        total += labels.size(0)\n",
                "        correct += predicted.eq(labels).sum().item()\n",
                "        \n",
                "        probs.append(F.softmax(outputs, dim=1).cpu().numpy())\n",
                "        \n",
                "    return running_loss / total, correct / total, np.concatenate(probs)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "32432b55",
            "metadata": {},
            "source": [
                "## 5. 5折交叉验证训练 (5-Fold CV)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "c23e1678",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "==================== Fold 1/5 ====================\n",
                        "[CNN] Ep 10 | Loss: 1.587 | Acc: 0.321 | Val Acc: 0.384\n",
                        "[CNN] Ep 20 | Loss: 1.384 | Acc: 0.471 | Val Acc: 0.375\n",
                        "[CNN] Ep 30 | Loss: 1.200 | Acc: 0.569 | Val Acc: 0.449\n",
                        "[CNN] Ep 40 | Loss: 1.093 | Acc: 0.647 | Val Acc: 0.449\n",
                        "[CNN] Fold 1 Best Acc: 0.5000\n"
                    ]
                },
                {
                    "ename": "NameError",
                    "evalue": "name 'predict' is not defined",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 71\u001b[39m\n\u001b[32m     69\u001b[39m test_ds_orig = IFT3395Dataset(X_test_raw, transform=val_transform)\n\u001b[32m     70\u001b[39m test_loader_orig = DataLoader(test_ds_orig, batch_size=BATCH_SIZE, shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers=\u001b[32m0\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m probs1 = \u001b[43mpredict\u001b[49m(model, test_loader_orig, device)\n\u001b[32m     73\u001b[39m \u001b[38;5;66;03m# You can add more TTA here if needed\u001b[39;00m\n\u001b[32m     74\u001b[39m cnn_test_probs += probs1 / N_FOLDS\n",
                        "\u001b[31mNameError\u001b[39m: name 'predict' is not defined"
                    ]
                }
            ],
            "source": [
                "N_FOLDS = 5\n",
                "EPOCHS = 40\n",
                "BATCH_SIZE = 32\n",
                "\n",
                "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
                "\n",
                "cnn_oof_probs = np.zeros((len(X_train_raw), 5))\n",
                "cnn_test_probs = np.zeros((len(X_test_raw), 5))\n",
                "et_oof_probs = np.zeros((len(X_train_raw), 5))\n",
                "et_test_probs = np.zeros((len(X_test_raw), 5))\n",
                "\n",
                "# ---------------------\n",
                "# SKLEARN PREP\n",
                "# ---------------------\n",
                "def flatten_norm(imgs):\n",
                "    return imgs.reshape(imgs.shape[0], -1).astype(np.float32) / 255.0\n",
                "\n",
                "X_train_flat = flatten_norm(X_train_raw)\n",
                "X_test_flat = flatten_norm(X_test_raw)\n",
                "\n",
                "for fold, (train_idx, val_idx) in enumerate(skf.split(X_train_raw, y_train)):\n",
                "    print(f\"\\n{'='*20} Fold {fold+1}/{N_FOLDS} {'='*20}\")\n",
                "    \n",
                "    # --- 1. Train PyTorch CNN ---\n",
                "    # Data Splitting\n",
                "    X_tr, y_tr = X_train_raw[train_idx], y_train[train_idx]\n",
                "    X_val, y_val = X_train_raw[val_idx], y_train[val_idx]\n",
                "    \n",
                "    # Datasets\n",
                "    # Tip: Use sampler for training to handle imbalance\n",
                "    sampler = get_sampler(y_tr)\n",
                "    train_ds = IFT3395Dataset(X_tr, y_tr, transform=train_transform)\n",
                "    val_ds = IFT3395Dataset(X_val, y_val, transform=val_transform)\n",
                "    \n",
                "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=sampler, num_workers=0)\n",
                "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
                "    \n",
                "    # Model Setup\n",
                "    model = CustomResNet9(num_classes=5).to(device)\n",
                "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1) # Label smoothing helps generalization\n",
                "    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
                "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
                "    \n",
                "    # Training Loop\n",
                "    best_acc = 0.0\n",
                "    best_weights = None\n",
                "    \n",
                "    for epoch in range(EPOCHS):\n",
                "        t_loss, t_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
                "        v_loss, v_acc, _ = validate(model, val_loader, criterion, device)\n",
                "        scheduler.step()\n",
                "        \n",
                "        if v_acc > best_acc:\n",
                "            best_acc = v_acc\n",
                "            best_weights = model.state_dict()\n",
                "            \n",
                "        if (epoch+1) % 10 == 0:\n",
                "            print(f\"[CNN] Ep {epoch+1} | Loss: {t_loss:.3f} | Acc: {t_acc:.3f} | Val Acc: {v_acc:.3f}\")\n",
                "            \n",
                "    # Load best & predict\n",
                "    model.load_state_dict(best_weights)\n",
                "    _, _, val_probs = validate(model, val_loader, criterion, device)\n",
                "    cnn_oof_probs[val_idx] = val_probs\n",
                "    print(f\"[CNN] Fold {fold+1} Best Acc: {best_acc:.4f}\")\n",
                "    \n",
                "    # TTA for Test Set\n",
                "    model.eval()\n",
                "    # Simple TTA: Original + Flip\n",
                "    test_ds_orig = IFT3395Dataset(X_test_raw, transform=val_transform)\n",
                "    test_loader_orig = DataLoader(test_ds_orig, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
                "    probs1 = predict(model, test_loader_orig, device)\n",
                "    \n",
                "    # You can add more TTA here if needed\n",
                "    cnn_test_probs += probs1 / N_FOLDS\n",
                "    \n",
                "    # --- 2. Train Sklearn ExtraTrees ---\n",
                "    # ExtraTrees is very robust for small numeric datasets\n",
                "    et = ExtraTreesClassifier(\n",
                "        n_estimators=300, \n",
                "        max_depth=None, \n",
                "        min_samples_split=2, \n",
                "        random_state=SEED,\n",
                "        class_weight='balanced', # Critical for imbalance\n",
                "        n_jobs=-1\n",
                "    )\n",
                "    \n",
                "    et.fit(X_train_flat[train_idx], y_train[train_idx])\n",
                "    et_val_acc = et.score(X_train_flat[val_idx], y_train[val_idx])\n",
                "    et_oof_probs[val_idx] = et.predict_proba(X_train_flat[val_idx])\n",
                "    et_test_probs += et.predict_proba(X_test_flat) / N_FOLDS\n",
                "    \n",
                "    print(f\"[ET]  Fold {fold+1} Acc: {et_val_acc:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. 集成与提交 Ensemble & Submission"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 计算独立的Accuracy\n",
                "cnn_acc = accuracy_score(y_train, np.argmax(cnn_oof_probs, axis=1))\n",
                "et_acc = accuracy_score(y_train, np.argmax(et_oof_probs, axis=1))\n",
                "\n",
                "print(f\"\\nOverall CV Accuracy:\")\n",
                "print(f\"CNN: {cnn_acc:.4f}\")\n",
                "print(f\"ExtraTrees: {et_acc:.4f}\")\n",
                "\n",
                "# 加权集成\n",
                "# 通常CNN比较自信，ET比较稳健，可以尝试不同的权重\n",
                "weights = [0.6, 0.4] # 0.6 CNN, 0.4 ET\n",
                "\n",
                "final_oof_probs = weights[0] * cnn_oof_probs + weights[1] * et_oof_probs\n",
                "ensemble_acc = accuracy_score(y_train, np.argmax(final_oof_probs, axis=1))\n",
                "print(f\"Ensemble: {ensemble_acc:.4f}\")\n",
                "\n",
                "# 生成最终预测\n",
                "final_test_probs = weights[0] * cnn_test_probs + weights[1] * et_test_probs\n",
                "predictions = np.argmax(final_test_probs, axis=1)\n",
                "\n",
                "# 保存CSV\n",
                "submission = pd.DataFrame({'ImageId': np.arange(len(predictions)), 'Label': predictions})\n",
                "submission.to_csv('submission_milestone2_ultimate.csv', index=False)\n",
                "print(\"\\nSubmission saved to submission_milestone2_ultimate.csv\")\n",
                "print(submission['Label'].value_counts())"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.14.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
