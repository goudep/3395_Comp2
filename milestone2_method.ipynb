{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# IFT3395 Competition 2 - Milestone 2: Advanced CNN Ensemble (PyTorch)\n",
        "\n",
        "目标：使用PyTorch构建CNN模型，结合数据增强、交叉验证集成和TTA，超越leaderboard最高分数0.54。\n",
        "\n",
        "## 核心策略\n",
        "1. **CNN架构**：使用卷积神经网络处理图像数据（比MLP更适合）\n",
        "2. **数据增强**：训练时随机翻转、旋转、颜色抖动等\n",
        "3. **交叉验证集成**：5折CV训练多个模型并集成\n",
        "4. **测试时增强（TTA）**：测试时使用多种增强模式提升预测稳定性\n",
        "5. **类别权重**：处理类别不平衡问题\n",
        "6. **早停机制**：防止过拟合\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 安装依赖库\n",
        "\n",
        "首先需要安装必要的Python库。如果已经安装，可以跳过此步骤。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 版本: 3.14.0\n",
            "系统: Windows 11\n",
            "\n",
            "✓ PyTorch 已安装且可用 (版本: 2.9.1+cpu)\n",
            "正在安装 scikit-learn...\n",
            "✓ scikit-learn 安装完成\n",
            "正在安装 Pillow...\n",
            "✓ Pillow 安装完成\n",
            "✓ NumPy 已安装 (版本: 2.3.5)\n",
            "\n",
            "==================================================\n",
            "✓ 所有依赖库检查完成！\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# 安装必要的依赖库\n",
        "# 如果已经安装，可以注释掉或跳过此cell\n",
        "\n",
        "import sys\n",
        "import subprocess\n",
        "import platform\n",
        "\n",
        "def install_package(package):\n",
        "    \"\"\"安装Python包\"\"\"\n",
        "    try:\n",
        "        __import__(package)\n",
        "        print(f\"✓ {package} 已安装\")\n",
        "        return True\n",
        "    except ImportError:\n",
        "        print(f\"正在安装 {package}...\")\n",
        "        try:\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"--quiet\"])\n",
        "            print(f\"✓ {package} 安装完成\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"✗ {package} 安装失败: {e}\")\n",
        "            return False\n",
        "\n",
        "def check_pytorch():\n",
        "    \"\"\"检查PyTorch是否可用\"\"\"\n",
        "    try:\n",
        "        import torch\n",
        "        # 尝试执行一个简单操作来验证PyTorch是否正常工作\n",
        "        _ = torch.tensor([1.0])\n",
        "        print(f\"✓ PyTorch 已安装且可用 (版本: {torch.__version__})\")\n",
        "        return True\n",
        "    except ImportError:\n",
        "        print(\"PyTorch 未安装\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"✗ PyTorch 导入失败: {type(e).__name__}: {e}\")\n",
        "        print(\"\\n⚠️  PyTorch DLL加载错误解决方案：\")\n",
        "        print(\"1. 卸载并重新安装PyTorch:\")\n",
        "        print(\"   pip uninstall torch torchvision -y\")\n",
        "        print(\"   pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu\")\n",
        "        print(\"\\n2. 如果问题仍然存在，可能需要安装Visual C++ Redistributable:\")\n",
        "        print(\"   https://aka.ms/vs/17/release/vc_redist.x64.exe\")\n",
        "        print(\"\\n3. 或者使用conda安装（如果可用）:\")\n",
        "        print(\"   conda install pytorch torchvision -c pytorch\")\n",
        "        return False\n",
        "\n",
        "# 检查Python版本\n",
        "python_version = sys.version_info\n",
        "print(f\"Python 版本: {python_version.major}.{python_version.minor}.{python_version.micro}\")\n",
        "print(f\"系统: {platform.system()} {platform.release()}\\n\")\n",
        "\n",
        "# 检查PyTorch\n",
        "pytorch_ok = check_pytorch()\n",
        "\n",
        "if not pytorch_ok:\n",
        "    print(\"\\n正在尝试重新安装PyTorch...\")\n",
        "    try:\n",
        "        # 先卸载\n",
        "        print(\"卸载旧版本...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"uninstall\", \"torch\", \"torchvision\", \"-y\"], \n",
        "                            stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    # 安装CPU版本（使用官方索引）\n",
        "    print(\"安装PyTorch CPU版本...\")\n",
        "    try:\n",
        "        subprocess.check_call([\n",
        "            sys.executable, \"-m\", \"pip\", \"install\", \n",
        "            \"torch\", \"torchvision\", \n",
        "            \"--index-url\", \"https://download.pytorch.org/whl/cpu\"\n",
        "        ])\n",
        "        print(\"✓ PyTorch 安装完成，请重新运行此cell验证\")\n",
        "    except Exception as e:\n",
        "        print(f\"✗ 安装失败: {e}\")\n",
        "        print(\"\\n请手动在命令行运行以下命令:\")\n",
        "        print(\"pip uninstall torch torchvision -y\")\n",
        "        print(\"pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu\")\n",
        "\n",
        "# 安装scikit-learn\n",
        "install_package(\"scikit-learn\")\n",
        "\n",
        "# 安装Pillow（PIL）\n",
        "install_package(\"Pillow\")\n",
        "\n",
        "# 安装numpy（通常已安装，但确保版本足够新）\n",
        "try:\n",
        "    import numpy as np\n",
        "    print(f\"✓ NumPy 已安装 (版本: {np.__version__})\")\n",
        "except ImportError:\n",
        "    install_package(\"numpy\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "if pytorch_ok or check_pytorch():\n",
        "    print(\"✓ 所有依赖库检查完成！\")\n",
        "else:\n",
        "    print(\"⚠️  PyTorch 可能仍有问题，请按照上述提示解决\")\n",
        "print(\"=\"*50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ PyTorch 可用，将使用CNN模型\n",
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import math\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from PIL import Image\n",
        "\n",
        "# 尝试导入PyTorch\n",
        "USE_PYTORCH = False\n",
        "try:\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "    import torch.nn.functional as F\n",
        "    import torch.optim as optim\n",
        "    from torch.utils.data import Dataset, DataLoader\n",
        "    from torchvision import transforms\n",
        "    \n",
        "    # 测试PyTorch是否真的可用\n",
        "    _ = torch.tensor([1.0])\n",
        "    USE_PYTORCH = True\n",
        "    print(\"✓ PyTorch 可用，将使用CNN模型\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠️  PyTorch 不可用 ({type(e).__name__})，将使用scikit-learn备用方案\")\n",
        "    print(\"   这可能是由于Python 3.14兼容性问题或DLL加载错误\")\n",
        "    print(\"   备用方案将使用RandomForest + ExtraTrees集成，性能仍然很好\")\n",
        "    USE_PYTORCH = False\n",
        "\n",
        "# 设置随机种子\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "\n",
        "if USE_PYTORCH:\n",
        "    torch.manual_seed(SEED)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(SEED)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f'Using device: {device}')\n",
        "else:\n",
        "    # 导入scikit-learn备用方案所需的库\n",
        "    from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, VotingClassifier\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    from sklearn.metrics import accuracy_score\n",
        "    print('Using scikit-learn ensemble method')\n",
        "\n",
        "DATA_DIR = Path('data')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: (1080, 28, 28, 3), Test: (400, 28, 28, 3), Classes: 5\n",
            "Image value range: [0.0, 255.0]\n",
            "Class weights: [0.29368767 1.11509536 0.69287478 0.73573302 2.16260918]\n"
          ]
        }
      ],
      "source": [
        "def load_split(split: str) -> Dict[str, np.ndarray]:\n",
        "    path = DATA_DIR / f\"{split}_data.pkl\"\n",
        "    with path.open('rb') as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "train_data = load_split('train')\n",
        "test_data = load_split('test')\n",
        "\n",
        "# 加载图像数据（原始数据可能是0-255范围的uint8或float32）\n",
        "images_raw = train_data['images']\n",
        "test_images_raw = test_data['images']\n",
        "\n",
        "# 转换为float32并确保在0-255范围（用于后续转换为PIL）\n",
        "if images_raw.dtype == np.uint8 or images_raw.max() > 1.0:\n",
        "    images = images_raw.astype(np.float32)\n",
        "    test_images = test_images_raw.astype(np.float32)\n",
        "else:\n",
        "    # 如果已经是0-1范围，转换为0-255范围\n",
        "    images = (images_raw * 255.0).astype(np.float32)\n",
        "    test_images = (test_images_raw * 255.0).astype(np.float32)\n",
        "\n",
        "labels = train_data['labels'].reshape(-1).astype(int)\n",
        "num_classes = len(np.unique(labels))\n",
        "print(f\"Train: {images.shape}, Test: {test_images.shape}, Classes: {num_classes}\")\n",
        "print(f\"Image value range: [{images.min():.1f}, {images.max():.1f}]\")\n",
        "\n",
        "# 计算类别权重\n",
        "unique, counts = np.unique(labels, return_counts=True)\n",
        "class_weights = counts.sum() / (counts.astype(np.float32) + 1e-6)\n",
        "class_weights = class_weights / class_weights.mean()\n",
        "print('Class weights:', class_weights)\n",
        "\n",
        "if USE_PYTORCH:\n",
        "    class_weights_tensor = torch.FloatTensor(class_weights).to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PyTorch CNN模型训练部分\n",
        "\n",
        "**⚠️ 重要提示**: \n",
        "- 如果PyTorch可用，请继续执行下面的所有cell\n",
        "- 如果PyTorch不可用（已在上方检测到），请**跳过**下面的PyTorch相关cell（Cell 6-17），直接跳到最后使用scikit-learn备用方案（Cell 19）\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not USE_PYTORCH:\n",
        "    print(\"⚠️ PyTorch不可用，请跳过此部分，直接跳到最后使用scikit-learn备用方案\")\n",
        "    raise RuntimeError(\"PyTorch不可用，请跳到最后使用scikit-learn备用方案（Cell 19）\")\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, images, labels=None, transform=None):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        img = self.images[idx].copy()\n",
        "        \n",
        "        # 确保图像在0-255范围内（处理float32的0-1范围或uint8的0-255范围）\n",
        "        if img.max() <= 1.0:\n",
        "            img = (img * 255).astype(np.uint8)\n",
        "        else:\n",
        "            img = img.astype(np.uint8)\n",
        "        \n",
        "        # 确保图像是HWC格式\n",
        "        if len(img.shape) == 3 and img.shape[2] == 3:\n",
        "            pass  # 已经是HWC格式\n",
        "        else:\n",
        "            raise ValueError(f\"Unexpected image shape: {img.shape}\")\n",
        "        \n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        else:\n",
        "            # 默认：转换为tensor并归一化\n",
        "            img = torch.from_numpy(img).permute(2, 0, 1).float() / 255.0\n",
        "        \n",
        "        if self.labels is not None:\n",
        "            return img, self.labels[idx]\n",
        "        return img\n",
        "\n",
        "# 训练时的数据增强\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomVerticalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# 验证/测试时的标准化\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# TTA变换列表（使用固定的变换，不使用随机）\n",
        "def fixed_rotation_10(img):\n",
        "    \"\"\"固定旋转10度\"\"\"\n",
        "    return img.rotate(10, resample=Image.BILINEAR)\n",
        "\n",
        "def fixed_rotation_minus_10(img):\n",
        "    \"\"\"固定旋转-10度\"\"\"\n",
        "    return img.rotate(-10, resample=Image.BILINEAR)\n",
        "\n",
        "tta_transforms = [\n",
        "    transforms.Compose([transforms.ToPILImage(), transforms.ToTensor(), \n",
        "                       transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]),  # 原始\n",
        "    transforms.Compose([transforms.ToPILImage(), transforms.Lambda(lambda x: x.transpose(Image.FLIP_LEFT_RIGHT)), \n",
        "                       transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]),  # 水平翻转\n",
        "    transforms.Compose([transforms.ToPILImage(), transforms.Lambda(lambda x: x.transpose(Image.FLIP_TOP_BOTTOM)), \n",
        "                       transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]),  # 垂直翻转\n",
        "    transforms.Compose([transforms.ToPILImage(), transforms.Lambda(fixed_rotation_10), \n",
        "                       transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]),  # 旋转+10度\n",
        "    transforms.Compose([transforms.ToPILImage(), transforms.Lambda(fixed_rotation_minus_10), \n",
        "                       transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]),  # 旋转-10度\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CNNClassifier(nn.Module):\n",
        "    def __init__(self, num_classes=5, dropout=0.3):\n",
        "        super(CNNClassifier, self).__init__()\n",
        "        \n",
        "        # 第一个卷积块\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "        self.dropout1 = nn.Dropout2d(dropout)\n",
        "        \n",
        "        # 第二个卷积块\n",
        "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "        self.conv4 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(64)\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "        self.dropout2 = nn.Dropout2d(dropout)\n",
        "        \n",
        "        # 第三个卷积块\n",
        "        self.conv5 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn5 = nn.BatchNorm2d(128)\n",
        "        self.conv6 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
        "        self.bn6 = nn.BatchNorm2d(128)\n",
        "        self.pool3 = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        \n",
        "        # 全连接层\n",
        "        self.fc1 = nn.Linear(128, 256)\n",
        "        self.bn_fc = nn.BatchNorm1d(256)\n",
        "        self.dropout_fc = nn.Dropout(dropout)\n",
        "        self.fc2 = nn.Linear(256, num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # 第一个卷积块\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.pool1(x)\n",
        "        x = self.dropout1(x)\n",
        "        \n",
        "        # 第二个卷积块\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = F.relu(self.bn4(self.conv4(x)))\n",
        "        x = self.pool2(x)\n",
        "        x = self.dropout2(x)\n",
        "        \n",
        "        # 第三个卷积块\n",
        "        x = F.relu(self.bn5(self.conv5(x)))\n",
        "        x = F.relu(self.bn6(self.conv6(x)))\n",
        "        x = self.pool3(x)\n",
        "        \n",
        "        # 展平\n",
        "        x = x.view(x.size(0), -1)\n",
        "        \n",
        "        # 全连接层\n",
        "        x = F.relu(self.bn_fc(self.fc1(x)))\n",
        "        x = self.dropout_fc(x)\n",
        "        x = self.fc2(x)\n",
        "        \n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        \n",
        "        # 梯度裁剪\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    \n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc = correct / total\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "def validate(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    \n",
        "    epoch_loss = running_loss / len(val_loader)\n",
        "    epoch_acc = correct / total\n",
        "    return epoch_loss, epoch_acc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 交叉验证训练\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "Fold 1/5\n",
            "==================================================\n",
            "Epoch 10/100 - Train Loss: 1.5923, Train Acc: 0.2951, Val Loss: 1.5361, Val Acc: 0.4769\n",
            "Epoch 20/100 - Train Loss: 1.5429, Train Acc: 0.3229, Val Loss: 1.5248, Val Acc: 0.2593\n",
            "Early stopping at epoch 25\n",
            "Fold 1 Best Val Acc: 0.4769\n",
            "\n",
            "==================================================\n",
            "Fold 2/5\n",
            "==================================================\n",
            "Epoch 10/100 - Train Loss: 1.5779, Train Acc: 0.3171, Val Loss: 1.5475, Val Acc: 0.3704\n",
            "Epoch 20/100 - Train Loss: 1.5406, Train Acc: 0.2998, Val Loss: 1.5639, Val Acc: 0.3241\n",
            "Epoch 30/100 - Train Loss: 1.5573, Train Acc: 0.2674, Val Loss: 1.5587, Val Acc: 0.3796\n",
            "Early stopping at epoch 36\n",
            "Fold 2 Best Val Acc: 0.4213\n",
            "\n",
            "==================================================\n",
            "Fold 3/5\n",
            "==================================================\n",
            "Epoch 10/100 - Train Loss: 1.5651, Train Acc: 0.3542, Val Loss: 1.5022, Val Acc: 0.3333\n",
            "Epoch 20/100 - Train Loss: 1.5366, Train Acc: 0.3368, Val Loss: 1.5288, Val Acc: 0.3333\n",
            "Epoch 30/100 - Train Loss: 1.5374, Train Acc: 0.3380, Val Loss: 1.5065, Val Acc: 0.3472\n",
            "Epoch 40/100 - Train Loss: 1.5271, Train Acc: 0.3958, Val Loss: 1.5088, Val Acc: 0.3796\n",
            "Early stopping at epoch 46\n",
            "Fold 3 Best Val Acc: 0.4954\n",
            "\n",
            "==================================================\n",
            "Fold 4/5\n",
            "==================================================\n",
            "Epoch 10/100 - Train Loss: 1.5682, Train Acc: 0.3079, Val Loss: 1.5523, Val Acc: 0.3380\n",
            "Epoch 20/100 - Train Loss: 1.5633, Train Acc: 0.3287, Val Loss: 1.5073, Val Acc: 0.3935\n",
            "Early stopping at epoch 28\n",
            "Fold 4 Best Val Acc: 0.3935\n",
            "\n",
            "==================================================\n",
            "Fold 5/5\n",
            "==================================================\n",
            "Epoch 10/100 - Train Loss: 1.5781, Train Acc: 0.3414, Val Loss: 1.5505, Val Acc: 0.3380\n",
            "Epoch 20/100 - Train Loss: 1.5803, Train Acc: 0.3206, Val Loss: 1.5275, Val Acc: 0.3565\n",
            "Early stopping at epoch 26\n",
            "Fold 5 Best Val Acc: 0.3657\n",
            "\n",
            "==================================================\n",
            "Mean Val Acc: 0.4306 ± 0.0490\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "K = 5\n",
        "skf = StratifiedKFold(n_splits=K, shuffle=True, random_state=SEED)\n",
        "\n",
        "fold_models = []\n",
        "fold_val_accs = []\n",
        "\n",
        "# 超参数\n",
        "params = {\n",
        "    'lr': 0.001,\n",
        "    'weight_decay': 1e-4,\n",
        "    'dropout': 0.3,\n",
        "    'batch_size': 32,\n",
        "    'epochs': 100,\n",
        "    'patience': 15\n",
        "}\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(images, labels)):\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Fold {fold + 1}/{K}\")\n",
        "    print(f\"{'='*50}\")\n",
        "    \n",
        "    # 准备数据\n",
        "    train_images_fold = images[train_idx]\n",
        "    train_labels_fold = labels[train_idx]\n",
        "    val_images_fold = images[val_idx]\n",
        "    val_labels_fold = labels[val_idx]\n",
        "    \n",
        "    train_dataset = ImageDataset(train_images_fold, train_labels_fold, transform=train_transform)\n",
        "    val_dataset = ImageDataset(val_images_fold, val_labels_fold, transform=val_transform)\n",
        "    \n",
        "    train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True, num_workers=0)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=params['batch_size'], shuffle=False, num_workers=0)\n",
        "    \n",
        "    # 创建模型\n",
        "    model = CNNClassifier(num_classes=num_classes, dropout=params['dropout']).to(device)\n",
        "    \n",
        "    # 损失函数（带类别权重）\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "    \n",
        "    # 优化器（使用余弦退火学习率调度）\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=params['epochs'], eta_min=1e-6)\n",
        "    \n",
        "    # 训练\n",
        "    best_val_acc = 0.0\n",
        "    best_model_state = None\n",
        "    patience_counter = 0\n",
        "    \n",
        "    for epoch in range(params['epochs']):\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
        "        scheduler.step()\n",
        "        \n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{params['epochs']} - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
        "                  f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "        \n",
        "        # 早停\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_model_state = model.state_dict().copy()\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= params['patience']:\n",
        "                print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "    \n",
        "    # 加载最佳模型\n",
        "    model.load_state_dict(best_model_state)\n",
        "    fold_models.append(model)\n",
        "    fold_val_accs.append(best_val_acc)\n",
        "    print(f\"Fold {fold + 1} Best Val Acc: {best_val_acc:.4f}\")\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"Mean Val Acc: {np.mean(fold_val_accs):.4f} ± {np.std(fold_val_accs):.4f}\")\n",
        "print(f\"{'='*50}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 全量数据训练额外模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training full model...\n",
            "Epoch 10/100 - Train Loss: 1.5561, Train Acc: 0.3261, Val Loss: 1.5457, Val Acc: 0.3426\n",
            "Early stopping at epoch 19\n",
            "Full model Best Val Acc: 0.3611\n",
            "Total models for ensemble: 6\n"
          ]
        }
      ],
      "source": [
        "# 使用90%训练，10%验证进行全量训练\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_images_full, val_images_full, train_labels_full, val_labels_full = train_test_split(\n",
        "    images, labels, test_size=0.1, random_state=SEED, stratify=labels\n",
        ")\n",
        "\n",
        "full_train_dataset = ImageDataset(train_images_full, train_labels_full, transform=train_transform)\n",
        "full_val_dataset = ImageDataset(val_images_full, val_labels_full, transform=val_transform)\n",
        "\n",
        "full_train_loader = DataLoader(full_train_dataset, batch_size=params['batch_size'], shuffle=True, num_workers=0)\n",
        "full_val_loader = DataLoader(full_val_dataset, batch_size=params['batch_size'], shuffle=False, num_workers=0)\n",
        "\n",
        "# 创建全量模型\n",
        "full_model = CNNClassifier(num_classes=num_classes, dropout=params['dropout']).to(device)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "optimizer = optim.AdamW(full_model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=params['epochs'], eta_min=1e-6)\n",
        "\n",
        "best_val_acc = 0.0\n",
        "best_model_state = None\n",
        "patience_counter = 0\n",
        "\n",
        "print(\"Training full model...\")\n",
        "for epoch in range(params['epochs']):\n",
        "    train_loss, train_acc = train_epoch(full_model, full_train_loader, criterion, optimizer, device)\n",
        "    val_loss, val_acc = validate(full_model, full_val_loader, criterion, device)\n",
        "    scheduler.step()\n",
        "    \n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{params['epochs']} - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "    \n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_model_state = full_model.state_dict().copy()\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= params['patience']:\n",
        "            print(f\"Early stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "full_model.load_state_dict(best_model_state)\n",
        "print(f\"Full model Best Val Acc: {best_val_acc:.4f}\")\n",
        "\n",
        "# 将所有模型加入列表\n",
        "all_models = fold_models + [full_model]\n",
        "print(f\"Total models for ensemble: {len(all_models)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 测试时增强（TTA）预测\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating predictions with TTA...\n",
            "Predictions shape: (400,)\n",
            "Prediction distribution: [164  20  64   1 151]\n"
          ]
        }
      ],
      "source": [
        "def predict_with_tta(models, test_images, tta_transforms, device, batch_size=32):\n",
        "    \"\"\"使用TTA进行预测\"\"\"\n",
        "    all_probs = []\n",
        "    \n",
        "    for model in models:\n",
        "        model.eval()\n",
        "        model_probs = []\n",
        "        \n",
        "        # 对每种TTA变换进行预测\n",
        "        for tta_transform in tta_transforms:\n",
        "            test_dataset = ImageDataset(test_images, transform=tta_transform)\n",
        "            test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "            \n",
        "            probs_list = []\n",
        "            with torch.no_grad():\n",
        "                for images in test_loader:\n",
        "                    images = images.to(device)\n",
        "                    outputs = model(images)\n",
        "                    probs = F.softmax(outputs, dim=1)\n",
        "                    probs_list.append(probs.cpu().numpy())\n",
        "            \n",
        "            model_probs.append(np.concatenate(probs_list, axis=0))\n",
        "        \n",
        "        # 对每种TTA变换的预测求平均\n",
        "        model_avg_probs = np.mean(model_probs, axis=0)\n",
        "        all_probs.append(model_avg_probs)\n",
        "    \n",
        "    # 对所有模型的预测求平均\n",
        "    ensemble_probs = np.mean(all_probs, axis=0)\n",
        "    predictions = np.argmax(ensemble_probs, axis=1)\n",
        "    \n",
        "    return predictions, ensemble_probs\n",
        "\n",
        "print(\"Generating predictions with TTA...\")\n",
        "test_predictions, test_probs = predict_with_tta(all_models, test_images, tta_transforms, device)\n",
        "print(f\"Predictions shape: {test_predictions.shape}\")\n",
        "print(f\"Prediction distribution: {np.bincount(test_predictions)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 生成提交文件\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 备用方案：使用scikit-learn（当PyTorch不可用时）\n",
        "\n",
        "如果PyTorch无法使用，以下代码将自动执行，使用scikit-learn的集成方法。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch可用，将使用CNN模型（见上方代码）\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# scikit-learn备用方案\n",
        "# ============================================================================\n",
        "\n",
        "if not USE_PYTORCH:\n",
        "    print(\"=\"*60)\n",
        "    print(\"使用scikit-learn备用方案\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # 数据预处理：展平图像\n",
        "    def flatten_images(imgs):\n",
        "        return imgs.reshape(len(imgs), -1).astype(np.float32) / 255.0\n",
        "    \n",
        "    X_train = flatten_images(images)\n",
        "    X_test = flatten_images(test_images)\n",
        "    \n",
        "    # 标准化\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    \n",
        "    print(f\"训练数据形状: {X_train_scaled.shape}\")\n",
        "    print(f\"测试数据形状: {X_test_scaled.shape}\")\n",
        "    \n",
        "    # 5折交叉验证训练多个模型\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
        "    fold_models = []\n",
        "    fold_scores = []\n",
        "    \n",
        "    print(\"\\n开始交叉验证训练...\")\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_train_scaled, labels)):\n",
        "        print(f\"\\nFold {fold + 1}/5\")\n",
        "        \n",
        "        X_train_fold = X_train_scaled[train_idx]\n",
        "        y_train_fold = labels[train_idx]\n",
        "        X_val_fold = X_train_scaled[val_idx]\n",
        "        y_val_fold = labels[val_idx]\n",
        "        \n",
        "        # 创建多个不同的模型\n",
        "        models = [\n",
        "            RandomForestClassifier(\n",
        "                n_estimators=300,\n",
        "                max_depth=20,\n",
        "                min_samples_split=5,\n",
        "                min_samples_leaf=2,\n",
        "                class_weight='balanced',\n",
        "                random_state=SEED + fold,\n",
        "                n_jobs=-1\n",
        "            ),\n",
        "            ExtraTreesClassifier(\n",
        "                n_estimators=300,\n",
        "                max_depth=20,\n",
        "                min_samples_split=5,\n",
        "                min_samples_leaf=2,\n",
        "                class_weight='balanced',\n",
        "                random_state=SEED + fold + 10,\n",
        "                n_jobs=-1\n",
        "            ),\n",
        "            RandomForestClassifier(\n",
        "                n_estimators=200,\n",
        "                max_depth=25,\n",
        "                min_samples_split=3,\n",
        "                min_samples_leaf=1,\n",
        "                class_weight='balanced',\n",
        "                random_state=SEED + fold + 20,\n",
        "                n_jobs=-1\n",
        "            )\n",
        "        ]\n",
        "        \n",
        "        # 训练每个模型\n",
        "        fold_predictions = []\n",
        "        for i, model in enumerate(models):\n",
        "            model.fit(X_train_fold, y_train_fold)\n",
        "            val_pred = model.predict(X_val_fold)\n",
        "            val_acc = accuracy_score(y_val_fold, val_pred)\n",
        "            fold_predictions.append(model.predict_proba(X_val_fold))\n",
        "            print(f\"  模型 {i+1} 验证准确率: {val_acc:.4f}\")\n",
        "        \n",
        "        # 集成预测（平均概率）\n",
        "        ensemble_probs = np.mean(fold_predictions, axis=0)\n",
        "        ensemble_pred = np.argmax(ensemble_probs, axis=1)\n",
        "        ensemble_acc = accuracy_score(y_val_fold, ensemble_pred)\n",
        "        print(f\"  集成验证准确率: {ensemble_acc:.4f}\")\n",
        "        \n",
        "        fold_models.append(models)\n",
        "        fold_scores.append(ensemble_acc)\n",
        "    \n",
        "    print(f\"\\n平均验证准确率: {np.mean(fold_scores):.4f} ± {np.std(fold_scores):.4f}\")\n",
        "    \n",
        "    # 在全量数据上训练最终模型\n",
        "    print(\"\\n训练全量模型...\")\n",
        "    final_models = [\n",
        "        RandomForestClassifier(\n",
        "            n_estimators=300, max_depth=20, min_samples_split=5, min_samples_leaf=2,\n",
        "            class_weight='balanced', random_state=SEED + 100, n_jobs=-1\n",
        "        ),\n",
        "        ExtraTreesClassifier(\n",
        "            n_estimators=300, max_depth=20, min_samples_split=5, min_samples_leaf=2,\n",
        "            class_weight='balanced', random_state=SEED + 110, n_jobs=-1\n",
        "        ),\n",
        "        RandomForestClassifier(\n",
        "            n_estimators=200, max_depth=25, min_samples_split=3, min_samples_leaf=1,\n",
        "            class_weight='balanced', random_state=SEED + 120, n_jobs=-1\n",
        "        )\n",
        "    ]\n",
        "    \n",
        "    for model in final_models:\n",
        "        model.fit(X_train_scaled, labels)\n",
        "    \n",
        "    all_models_sklearn = []\n",
        "    for fold_models_list in fold_models:\n",
        "        all_models_sklearn.extend(fold_models_list)\n",
        "    all_models_sklearn.extend(final_models)\n",
        "    \n",
        "    print(f\"总模型数: {len(all_models_sklearn)}\")\n",
        "    \n",
        "    # 对测试集进行预测\n",
        "    print(\"\\n生成测试集预测...\")\n",
        "    test_probs_list = []\n",
        "    for model in all_models_sklearn:\n",
        "        test_probs_list.append(model.predict_proba(X_test_scaled))\n",
        "    \n",
        "    # 集成所有模型的预测\n",
        "    test_ensemble_probs = np.mean(test_probs_list, axis=0)\n",
        "    test_predictions = np.argmax(test_ensemble_probs, axis=1)\n",
        "    \n",
        "    print(f\"预测分布: {np.bincount(test_predictions)}\")\n",
        "    \n",
        "    # 生成提交文件\n",
        "    submission_path = Path('submission_milestone2.csv')\n",
        "    with submission_path.open('w', newline='') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['ID', 'Label'])\n",
        "        for idx, label in enumerate(test_predictions, start=1):\n",
        "            writer.writerow([str(idx), int(label)])\n",
        "    \n",
        "    print(f'\\n提交文件已保存: {submission_path.resolve()}')\n",
        "    print(f'总预测数: {len(test_predictions)}')\n",
        "    \n",
        "else:\n",
        "    print(\"PyTorch可用，将使用CNN模型（见上方代码）\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Submission saved to C:\\Users\\yudim\\Downloads\\IFT3395_Competition2\\submission_milestone2.csv\n",
            "Total predictions: 400\n"
          ]
        }
      ],
      "source": [
        "submission_path = Path('submission_milestone2.csv')\n",
        "with submission_path.open('w', newline='') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(['ID', 'Label'])\n",
        "    for idx, label in enumerate(test_predictions, start=1):\n",
        "        writer.writerow([str(idx), int(label)])\n",
        "\n",
        "print(f'Submission saved to {submission_path.resolve()}')\n",
        "print(f'Total predictions: {len(test_predictions)}')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
