{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Kaggle Diabetic Retinopathy - Milestone 2 (PyTorch CNN)\n",
                "\n",
                "In this notebook, we implement a **Convolutional Neural Network (CNN)** using **PyTorch** to improve upon the Milestone 1 baseline.\n",
                "We use data augmentation to handle the small dataset size and train a custom CNN architecture.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Using device: cpu\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import Dataset, DataLoader, random_split\n",
                "import torchvision.transforms as transforms\n",
                "import numpy as np\n",
                "import pickle\n",
                "from PIL import Image\n",
                "import matplotlib.pyplot as plt\n",
                "import pandas as pd\n",
                "import os\n",
                "# Check device\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Using device: {device}\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Dataset Implementation\n",
                "We define a custom `Dataset` class to load the pickle data and apply transformations.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "class RetinopathyDataset(Dataset):\n",
                "    def __init__(self, pickle_path, transform=None, mode='train'):\n",
                "        self.transform = transform\n",
                "        self.mode = mode\n",
                "        \n",
                "        with open(pickle_path, 'rb') as f:\n",
                "            data = pickle.load(f)\n",
                "            \n",
                "        self.images = data['images']\n",
                "        if self.mode == 'train':\n",
                "            self.labels = data['labels'].flatten()\n",
                "        else:\n",
                "            self.labels = None\n",
                "            \n",
                "    def __len__(self):\n",
                "        return len(self.images)\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        img = self.images[idx]\n",
                "        img = Image.fromarray(img) # Convert to PIL for transforms\n",
                "        \n",
                "        if self.transform:\n",
                "            img = self.transform(img)\n",
                "            \n",
                "        if self.mode == 'train':\n",
                "            label = self.labels[idx]\n",
                "            return img, torch.tensor(label, dtype=torch.long)\n",
                "        else:\n",
                "            return img\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Data Augmentation & Loading\n",
                "We apply aggressive data augmentation (flips, rotations, color jitter) to prevent overfitting.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "ename": "FileNotFoundError",
                    "evalue": "[Errno 2] No such file or directory: 'train_data.pkl'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     15\u001b[39m val_transform = transforms.Compose([\n\u001b[32m     16\u001b[39m     transforms.ToTensor(),\n\u001b[32m     17\u001b[39m     transforms.Normalize((\u001b[32m0.5\u001b[39m, \u001b[32m0.5\u001b[39m, \u001b[32m0.5\u001b[39m), (\u001b[32m0.5\u001b[39m, \u001b[32m0.5\u001b[39m, \u001b[32m0.5\u001b[39m))\n\u001b[32m     18\u001b[39m ])\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Load Data\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m full_dataset = \u001b[43mRetinopathyDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain_data.pkl\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_transform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Split\u001b[39;00m\n\u001b[32m     24\u001b[39m torch.manual_seed(SEED)\n",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mRetinopathyDataset.__init__\u001b[39m\u001b[34m(self, pickle_path, transform, mode)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mself\u001b[39m.transform = transform\n\u001b[32m      4\u001b[39m \u001b[38;5;28mself\u001b[39m.mode = mode\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpickle_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      7\u001b[39m     data = pickle.load(f)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mself\u001b[39m.images = data[\u001b[33m'\u001b[39m\u001b[33mimages\u001b[39m\u001b[33m'\u001b[39m]\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yudim\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:343\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    338\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    339\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'train_data.pkl'"
                    ]
                }
            ],
            "source": [
                "# Hyperparameters\n",
                "BATCH_SIZE = 32\n",
                "VAL_SPLIT = 0.2\n",
                "SEED = 42\n",
                "\n",
                "# Transforms\n",
                "train_transform = transforms.Compose([\n",
                "    transforms.RandomHorizontalFlip(),\n",
                "    transforms.RandomRotation(10),\n",
                "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
                "])\n",
                "\n",
                "val_transform = transforms.Compose([\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
                "])\n",
                "\n",
                "# Load Data\n",
                "full_dataset = RetinopathyDataset('train_data.pkl', transform=train_transform, mode='train')\n",
                "\n",
                "# Split\n",
                "torch.manual_seed(SEED)\n",
                "indices = torch.randperm(len(full_dataset)).tolist()\n",
                "val_size = int(len(full_dataset) * VAL_SPLIT)\n",
                "train_indices = indices[val_size:]\n",
                "val_indices = indices[:val_size]\n",
                "\n",
                "# Create Subsets with correct transforms\n",
                "# Note: We re-instantiate to ensure validation set has no augmentation\n",
                "train_subset = torch.utils.data.Subset(\n",
                "    RetinopathyDataset('train_data.pkl', transform=train_transform, mode='train'), \n",
                "    train_indices\n",
                ")\n",
                "val_subset = torch.utils.data.Subset(\n",
                "    RetinopathyDataset('train_data.pkl', transform=val_transform, mode='train'), \n",
                "    val_indices\n",
                ")\n",
                "\n",
                "train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True)\n",
                "val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False)\n",
                "\n",
                "print(f\"Train batches: {len(train_loader)}\")\n",
                "print(f\"Val batches: {len(val_loader)}\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Model Architecture\n",
                "A simple CNN with 2 convolutional layers, max pooling, and dropout.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "SimpleCNN(\n",
                        "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
                        "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                        "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
                        "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
                        "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                        "  (fc1): Linear(in_features=3136, out_features=256, bias=True)\n",
                        "  (dropout): Dropout(p=0.5, inplace=False)\n",
                        "  (fc2): Linear(in_features=256, out_features=5, bias=True)\n",
                        ")\n"
                    ]
                }
            ],
            "source": [
                "class SimpleCNN(nn.Module):\n",
                "    def __init__(self, num_classes=5):\n",
                "        super(SimpleCNN, self).__init__()\n",
                "        \n",
                "        # Input: (3, 28, 28)\n",
                "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
                "        self.bn1 = nn.BatchNorm2d(32)\n",
                "        self.pool = nn.MaxPool2d(2, 2) # Output: (32, 14, 14)\n",
                "        \n",
                "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
                "        self.bn2 = nn.BatchNorm2d(64)\n",
                "        # Pool again -> Output: (64, 7, 7)\n",
                "        \n",
                "        self.flatten_dim = 64 * 7 * 7\n",
                "        \n",
                "        self.fc1 = nn.Linear(self.flatten_dim, 256)\n",
                "        self.dropout = nn.Dropout(0.5)\n",
                "        self.fc2 = nn.Linear(256, num_classes)\n",
                "        \n",
                "    def forward(self, x):\n",
                "        x = self.pool(torch.relu(self.bn1(self.conv1(x))))\n",
                "        x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n",
                "        x = x.view(-1, self.flatten_dim)\n",
                "        x = torch.relu(self.fc1(x))\n",
                "        x = self.dropout(x)\n",
                "        x = self.fc2(x)\n",
                "        return x\n",
                "\n",
                "model = SimpleCNN(num_classes=5).to(device)\n",
                "print(model)\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Training Loop\n",
                "We train for 200 epochs and save the best model based on validation accuracy.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Starting training for 200 epochs...\n"
                    ]
                },
                {
                    "ename": "NameError",
                    "evalue": "name 'train_loader' is not defined",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m correct = \u001b[32m0\u001b[39m\n\u001b[32m     15\u001b[39m total = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtrain_loader\u001b[49m:\n\u001b[32m     18\u001b[39m     images, labels = images.to(device), labels.to(device)\n\u001b[32m     20\u001b[39m     optimizer.zero_grad()\n",
                        "\u001b[31mNameError\u001b[39m: name 'train_loader' is not defined"
                    ]
                }
            ],
            "source": [
                "criterion = nn.CrossEntropyLoss()\n",
                "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
                "\n",
                "EPOCHS = 200\n",
                "best_val_acc = 0.0\n",
                "train_losses = []\n",
                "val_accs = []\n",
                "\n",
                "print(f\"Starting training for {EPOCHS} epochs...\")\n",
                "\n",
                "for epoch in range(EPOCHS):\n",
                "    model.train()\n",
                "    running_loss = 0.0\n",
                "    correct = 0\n",
                "    total = 0\n",
                "    \n",
                "    for images, labels in train_loader:\n",
                "        images, labels = images.to(device), labels.to(device)\n",
                "        \n",
                "        optimizer.zero_grad()\n",
                "        outputs = model(images)\n",
                "        loss = criterion(outputs, labels)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        \n",
                "        running_loss += loss.item()\n",
                "        _, predicted = torch.max(outputs.data, 1)\n",
                "        total += labels.size(0)\n",
                "        correct += (predicted == labels).sum().item()\n",
                "        \n",
                "    train_acc = 100 * correct / total\n",
                "    avg_loss = running_loss / len(train_loader)\n",
                "    train_losses.append(avg_loss)\n",
                "    \n",
                "    # Validation\n",
                "    model.eval()\n",
                "    val_correct = 0\n",
                "    val_total = 0\n",
                "    with torch.no_grad():\n",
                "        for images, labels in val_loader:\n",
                "            images, labels = images.to(device), labels.to(device)\n",
                "            outputs = model(images)\n",
                "            _, predicted = torch.max(outputs.data, 1)\n",
                "            val_total += labels.size(0)\n",
                "            val_correct += (predicted == labels).sum().item()\n",
                "            \n",
                "    val_acc = 100 * val_correct / val_total\n",
                "    val_accs.append(val_acc)\n",
                "    \n",
                "    if val_acc > best_val_acc:\n",
                "        best_val_acc = val_acc\n",
                "        torch.save(model.state_dict(), 'best_model_notebook.pth')\n",
                "        \n",
                "    if (epoch + 1) % 10 == 0:\n",
                "        print(f\"Epoch [{epoch+1}/{EPOCHS}], Loss: {avg_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%\")\n",
                "\n",
                "print(f\"Best Validation Accuracy: {best_val_acc:.2f}%\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Visualization\n",
                "Plotting the training loss and validation accuracy.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(12, 5))\n",
                "plt.subplot(1, 2, 1)\n",
                "plt.plot(train_losses, label='Train Loss')\n",
                "plt.title('Training Loss')\n",
                "plt.xlabel('Epoch')\n",
                "plt.ylabel('Loss')\n",
                "plt.legend()\n",
                "\n",
                "plt.subplot(1, 2, 2)\n",
                "plt.plot(val_accs, label='Val Accuracy', color='orange')\n",
                "plt.title('Validation Accuracy')\n",
                "plt.xlabel('Epoch')\n",
                "plt.ylabel('Accuracy (%)')\n",
                "plt.legend()\n",
                "\n",
                "plt.show()\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Submission\n",
                "Generate predictions for the test set using the best model.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Test Data\n",
                "test_dataset = RetinopathyDataset('test_data.pkl', transform=val_transform, mode='test')\n",
                "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
                "\n",
                "# Load Best Model\n",
                "model.load_state_dict(torch.load('best_model_notebook.pth'))\n",
                "model.eval()\n",
                "\n",
                "predictions = []\n",
                "with torch.no_grad():\n",
                "    for images in test_loader:\n",
                "        images = images.to(device)\n",
                "        outputs = model(images)\n",
                "        _, predicted = torch.max(outputs.data, 1)\n",
                "        predictions.extend(predicted.cpu().numpy())\n",
                "\n",
                "# Create Submission CSV\n",
                "ids = np.arange(1, len(predictions) + 1)\n",
                "df = pd.DataFrame({'ID': ids, 'Label': predictions})\n",
                "filename = 'submission_milestone2_notebook.csv'\n",
                "df.to_csv(filename, index=False)\n",
                "\n",
                "print(f\"Submission saved to {filename}\")\n",
                "df.head()\n",
                "\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.14.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
