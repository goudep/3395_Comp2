{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# IFT3395 Competition 2 - Milestone 2 v4.3\n",
                "\n",
                "## 目标: Validation Accuracy > 0.53, 训练时间 < 15分钟\n",
                "\n",
                "## 混合策略 (Hybrid Strategy)\n",
                "结合 `milestone2_v4.2` 的 **Ensemble/Mixup** 框架与 `Milestone2.ipynb` 的 **SimpleCNN** 核心。\n",
                "\n",
                "- **Architecture**: SimpleCNN (2-layer, 32->64 filters) - *From Milestone2.ipynb*\n",
                "- **Normalization**: `(0.5, 0.5, 0.5)` - *From Milestone2.ipynb*\n",
                "- **Training**: 60 Epochs (SimpleCNN is fast), OneCycleLR\n",
                "- **Ensemble**: CNN + ET + RF + HGB + SVC (Diverse 5-model stack)\n",
                "- **Augmentation**: Mixup + Lightweight TTA"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Device: cpu\n"
                    ]
                }
            ],
            "source": [
                "# Cell 1: Imports\n",
                "import pickle\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from pathlib import Path\n",
                "import random, os, warnings, time\n",
                "from PIL import Image\n",
                "\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
                "from torchvision import transforms\n",
                "\n",
                "from sklearn.model_selection import StratifiedKFold\n",
                "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, HistGradientBoostingClassifier\n",
                "from sklearn.svm import SVC\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.pipeline import make_pipeline\n",
                "from sklearn.metrics import accuracy_score\n",
                "\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "SEED = 42\n",
                "def seed_all(seed):\n",
                "    random.seed(seed)\n",
                "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
                "    np.random.seed(seed)\n",
                "    torch.manual_seed(seed)\n",
                "    if torch.cuda.is_available():\n",
                "        torch.cuda.manual_seed(seed)\n",
                "        torch.backends.cudnn.deterministic = True\n",
                "\n",
                "seed_all(SEED)\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Device: {device}\")\n",
                "t0 = time.time()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Train: (1080, 28, 28, 3), Test: (400, 28, 28, 3)\n",
                        "Classes: [486 128 206 194  66]\n"
                    ]
                }
            ],
            "source": [
                "# Cell 2: Load Data\n",
                "DATA_DIR = Path('data')\n",
                "with open(DATA_DIR / 'train_data.pkl', 'rb') as f:\n",
                "    train_data = pickle.load(f)\n",
                "with open(DATA_DIR / 'test_data.pkl', 'rb') as f:\n",
                "    test_data = pickle.load(f)\n",
                "\n",
                "X_train = train_data['images']\n",
                "y_train = train_data['labels'].flatten().astype(np.int64)\n",
                "X_test = test_data['images']\n",
                "\n",
                "if X_train.max() <= 1.0:\n",
                "    X_train = (X_train * 255).astype(np.uint8)\n",
                "    X_test = (X_test * 255).astype(np.uint8)\n",
                "else:\n",
                "    X_train = X_train.astype(np.uint8)\n",
                "    X_test = X_test.astype(np.uint8)\n",
                "\n",
                "n_train, n_test = len(X_train), len(X_test)\n",
                "print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")\n",
                "print(f\"Classes: {np.bincount(y_train)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Extracting features...\n",
                        "Feature shape: (1080, 2374)\n"
                    ]
                }
            ],
            "source": [
                "# Cell 3: Feature Engineering (For Sklearn Models)\n",
                "def extract_features(images):\n",
                "    \"\"\"Extract features: flatten pixels + color statistics\"\"\"\n",
                "    features = []\n",
                "    for img in images:\n",
                "        # Flatten RGB pixels\n",
                "        flat = img.flatten().astype(np.float32) / 255.0\n",
                "        \n",
                "        # Color statistics per channel\n",
                "        stats = []\n",
                "        for c in range(3):\n",
                "            ch = img[:, :, c].astype(np.float32)\n",
                "            stats.extend([ch.mean(), ch.std(), ch.min(), ch.max(),\n",
                "                          np.percentile(ch, 25), np.percentile(ch, 75)])\n",
                "        \n",
                "        # Grayscale stats\n",
                "        gray = 0.299*img[:,:,0] + 0.587*img[:,:,1] + 0.114*img[:,:,2]\n",
                "        stats.extend([gray.mean(), gray.std(), gray.min(), gray.max()])\n",
                "        \n",
                "        # Combine\n",
                "        features.append(np.concatenate([flat, np.array(stats, dtype=np.float32)]))\n",
                "    return np.array(features, dtype=np.float32)\n",
                "\n",
                "print(\"Extracting features...\")\n",
                "X_train_feat = extract_features(X_train)\n",
                "X_test_feat = extract_features(X_test)\n",
                "print(f\"Feature shape: {X_train_feat.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model defined.\n"
                    ]
                }
            ],
            "source": [
                "# Cell 4: Dataset & SimpleCNN Model\n",
                "class ImgDataset(Dataset):\n",
                "    def __init__(self, images, labels=None, transform=None):\n",
                "        self.images = images\n",
                "        self.labels = labels\n",
                "        self.transform = transform\n",
                "        \n",
                "    def __len__(self): return len(self.images)\n",
                "    \n",
                "    def __getitem__(self, i):\n",
                "        img = Image.fromarray(self.images[i].astype(np.uint8))\n",
                "        if self.transform: img = self.transform(img)\n",
                "        if self.labels is not None:\n",
                "            return img, torch.tensor(self.labels[i], dtype=torch.long)\n",
                "        return img\n",
                "\n",
                "# TRANSFORMS: Optimized to match Milestone2.ipynb stats (0.5, 0.5, 0.5)\n",
                "train_tf = transforms.Compose([\n",
                "    transforms.RandomHorizontalFlip(),\n",
                "    transforms.RandomRotation(15),\n",
                "    transforms.ColorJitter(0.2, 0.2, 0.2),\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
                "])\n",
                "val_tf = transforms.Compose([\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
                "])\n",
                "\n",
                "# SimpleCNN from Milestone2.ipynb\n",
                "# Architecture: 32 -> 64 -> Dense\n",
                "class SimpleCNN(nn.Module):\n",
                "    def __init__(self, num_classes=5):\n",
                "        super(SimpleCNN, self).__init__()\n",
                "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
                "        self.bn1 = nn.BatchNorm2d(32)\n",
                "        self.pool = nn.MaxPool2d(2, 2) \n",
                "        \n",
                "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
                "        self.bn2 = nn.BatchNorm2d(64)\n",
                "        \n",
                "        self.flatten_dim = 64 * 7 * 7 \n",
                "        \n",
                "        self.fc1 = nn.Linear(self.flatten_dim, 256)\n",
                "        self.dropout = nn.Dropout(0.5)\n",
                "        self.fc2 = nn.Linear(256, num_classes)\n",
                "        \n",
                "    def forward(self, x):\n",
                "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
                "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
                "        x = x.reshape(-1, self.flatten_dim)\n",
                "        x = F.relu(self.fc1(x))\n",
                "        x = self.dropout(x)\n",
                "        x = self.fc2(x)\n",
                "        return x\n",
                "\n",
                "print(\"Model defined.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Helpers defined.\n"
                    ]
                }
            ],
            "source": [
                "# Cell 5: Helpers with Mixup and TTA\n",
                "def get_sampler(y):\n",
                "    w = 1.0 / np.maximum(np.bincount(y), 1)\n",
                "    return WeightedRandomSampler(torch.from_numpy(w[y]).double(), len(y))\n",
                "\n",
                "def mixup_data(x, y, alpha=0.4, device='cuda'):\n",
                "    if alpha > 0:\n",
                "        lam = np.random.beta(alpha, alpha)\n",
                "    else:\n",
                "        lam = 1\n",
                "    batch_size = x.size(0)\n",
                "    index = torch.randperm(batch_size).to(device)\n",
                "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
                "    y_a, y_b = y, y[index]\n",
                "    return mixed_x, y_a, y_b, lam\n",
                "\n",
                "def train_ep(model, loader, crit, opt, dev, use_mixup=True):\n",
                "    model.train()\n",
                "    for x, y in loader:\n",
                "        x, y = x.to(dev), y.to(dev)\n",
                "        opt.zero_grad()\n",
                "        \n",
                "        if use_mixup:\n",
                "            x, y_a, y_b, lam = mixup_data(x, y, 0.4, dev)\n",
                "            out = model(x)\n",
                "            loss = lam * crit(out, y_a) + (1 - lam) * crit(out, y_b)\n",
                "        else:\n",
                "            loss = crit(model(x), y)\n",
                "            \n",
                "        loss.backward()\n",
                "        opt.step()\n",
                "\n",
                "@torch.no_grad()\n",
                "def val_ep(model, loader, dev):\n",
                "    model.eval()\n",
                "    c, t, probs = 0, 0, []\n",
                "    for x, y in loader:\n",
                "        x, y = x.to(dev), y.to(dev)\n",
                "        out = model(x)\n",
                "        c += (out.argmax(1) == y).sum().item()\n",
                "        t += y.size(0)\n",
                "        probs.append(F.softmax(out, 1).cpu().numpy())\n",
                "    return c / t, np.concatenate(probs)\n",
                "\n",
                "@torch.no_grad()\n",
                "def predict_tta(model, images, dev, tta=True):\n",
                "    model.eval()\n",
                "    # 1. Original\n",
                "    ds = ImgDataset(images, None, val_tf)\n",
                "    loader = DataLoader(ds, 64, shuffle=False, num_workers=0)\n",
                "    probs = []\n",
                "    for x in loader:\n",
                "        probs.append(F.softmax(model(x.to(dev)), 1).cpu().numpy())\n",
                "    p1 = np.concatenate(probs)\n",
                "    \n",
                "    if not tta: return p1\n",
                "    \n",
                "    # 2. Horizontal Flip\n",
                "    # Note: val_tf normalizes, so we need to flip BEFORE normalization or use flip tensor\n",
                "    # Safest is to flip numpy, then dataset process it\n",
                "    images_flip = np.array([np.fliplr(img) for img in images])\n",
                "    ds_flip = ImgDataset(images_flip, None, val_tf)\n",
                "    loader_flip = DataLoader(ds_flip, 64, shuffle=False, num_workers=0)\n",
                "    probs_flip = []\n",
                "    for x in loader_flip:\n",
                "        probs_flip.append(F.softmax(model(x.to(dev)), 1).cpu().numpy())\n",
                "    p2 = np.concatenate(probs_flip)\n",
                "    \n",
                "    return 0.5 * (p1 + p2)\n",
                "\n",
                "print(\"Helpers defined.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Training: 5 folds, 60 epochs, Mixup=True, Model=SimpleCNN\n",
                        "==================================================\n",
                        "\n",
                        "Fold 1/5\n",
                        "  CNN: 0.5000\n",
                        "  ET:  0.5324\n",
                        "  RF:  0.4815\n",
                        "  HGB: 0.5231\n",
                        "  SVC: 0.5278\n",
                        "  Time: 2.4min\n",
                        "\n",
                        "Fold 2/5\n",
                        "  CNN: 0.4583\n",
                        "  ET:  0.5139\n",
                        "  RF:  0.5000\n",
                        "  HGB: 0.4815\n",
                        "  SVC: 0.4815\n",
                        "  Time: 2.7min\n",
                        "\n",
                        "Fold 3/5\n",
                        "  CNN: 0.5093\n",
                        "  ET:  0.5185\n",
                        "  RF:  0.5231\n",
                        "  HGB: 0.5231\n",
                        "  SVC: 0.4954\n",
                        "  Time: 2.8min\n",
                        "\n",
                        "Fold 4/5\n",
                        "  CNN: 0.5139\n",
                        "  ET:  0.4907\n",
                        "  RF:  0.5000\n",
                        "  HGB: 0.4954\n",
                        "  SVC: 0.5185\n",
                        "  Time: 2.8min\n",
                        "\n",
                        "Fold 5/5\n",
                        "  CNN: 0.4583\n",
                        "  ET:  0.4722\n",
                        "  RF:  0.5093\n",
                        "  HGB: 0.4491\n",
                        "  SVC: 0.4537\n",
                        "  Time: 2.9min\n",
                        "\n",
                        "Total: 13.8min\n"
                    ]
                }
            ],
            "source": [
                "# Cell 6: Training\n",
                "N_FOLDS, EPOCHS, BS = 5, 60, 64  # Increased Epochs for simpler model\n",
                "skf = StratifiedKFold(N_FOLDS, shuffle=True, random_state=SEED)\n",
                "\n",
                "# Storage\n",
                "cnn_oof = np.zeros((n_train, 5))\n",
                "cnn_test = np.zeros((n_test, 5))\n",
                "et_oof = np.zeros((n_train, 5))\n",
                "et_test = np.zeros((n_test, 5))\n",
                "rf_oof = np.zeros((n_train, 5))\n",
                "rf_test = np.zeros((n_test, 5))\n",
                "hgb_oof = np.zeros((n_train, 5))\n",
                "hgb_test = np.zeros((n_test, 5))\n",
                "svc_oof = np.zeros((n_train, 5))\n",
                "svc_test = np.zeros((n_test, 5))\n",
                "\n",
                "print(f\"Training: {N_FOLDS} folds, {EPOCHS} epochs, Mixup=True, Model=SimpleCNN\")\n",
                "print(\"=\"*50)\n",
                "\n",
                "for fold, (tr_i, val_i) in enumerate(skf.split(X_train, y_train)):\n",
                "    t1 = time.time()\n",
                "    print(f\"\\nFold {fold+1}/{N_FOLDS}\")\n",
                "    \n",
                "    # CNN (SimpleCNN)\n",
                "    tr_ds = ImgDataset(X_train[tr_i], y_train[tr_i], train_tf)\n",
                "    val_ds = ImgDataset(X_train[val_i], y_train[val_i], val_tf)\n",
                "    tr_ld = DataLoader(tr_ds, BS, sampler=get_sampler(y_train[tr_i]), num_workers=0)\n",
                "    val_ld = DataLoader(val_ds, BS, shuffle=False, num_workers=0)\n",
                "    \n",
                "    model = SimpleCNN().to(device)\n",
                "    crit = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
                "    opt = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
                "    sch = optim.lr_scheduler.OneCycleLR(opt, max_lr=0.003, steps_per_epoch=len(tr_ld), epochs=EPOCHS)\n",
                "    \n",
                "    best_acc, best_w = 0, None\n",
                "    for ep in range(EPOCHS):\n",
                "        train_ep(model, tr_ld, crit, opt, device, use_mixup=True)\n",
                "        acc, _ = val_ep(model, val_ld, device)\n",
                "        sch.step()\n",
                "        if acc > best_acc:\n",
                "            best_acc = acc\n",
                "            best_w = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
                "    \n",
                "    model.load_state_dict(best_w)\n",
                "    _, probs = val_ep(model, val_ld, device)\n",
                "    cnn_oof[val_i] = probs\n",
                "    cnn_test += predict_tta(model, X_test, device, tta=True) / N_FOLDS\n",
                "    print(f\"  CNN: {best_acc:.4f}\")\n",
                "    \n",
                "    # Sklearn Models with cached features\n",
                "    X_tr_f, y_tr_f = X_train_feat[tr_i], y_train[tr_i]\n",
                "    X_val_f = X_train_feat[val_i]\n",
                "    \n",
                "    # ET\n",
                "    et = ExtraTreesClassifier(200, max_depth=25, class_weight='balanced', random_state=SEED+fold, n_jobs=-1)\n",
                "    et.fit(X_tr_f, y_tr_f)\n",
                "    et_oof[val_i] = et.predict_proba(X_val_f)\n",
                "    et_test += et.predict_proba(X_test_feat) / N_FOLDS\n",
                "    print(f\"  ET:  {et.score(X_val_f, y_train[val_i]):.4f}\")\n",
                "    \n",
                "    # RF\n",
                "    rf = RandomForestClassifier(200, max_depth=25, class_weight='balanced', random_state=SEED+fold, n_jobs=-1)\n",
                "    rf.fit(X_tr_f, y_tr_f)\n",
                "    rf_oof[val_i] = rf.predict_proba(X_val_f)\n",
                "    rf_test += rf.predict_proba(X_test_feat) / N_FOLDS\n",
                "    print(f\"  RF:  {rf.score(X_val_f, y_train[val_i]):.4f}\")\n",
                "    \n",
                "    # HGB\n",
                "    hgb = HistGradientBoostingClassifier(max_iter=100, max_depth=10, random_state=SEED+fold)\n",
                "    hgb.fit(X_tr_f, y_tr_f)\n",
                "    hgb_oof[val_i] = hgb.predict_proba(X_val_f)\n",
                "    hgb_test += hgb.predict_proba(X_test_feat) / N_FOLDS\n",
                "    print(f\"  HGB: {hgb.score(X_val_f, y_train[val_i]):.4f}\")\n",
                "    \n",
                "    # SVC\n",
                "    svc = make_pipeline(StandardScaler(), SVC(probability=True, class_weight='balanced', kernel='rbf', C=10))\n",
                "    svc.fit(X_tr_f, y_tr_f)\n",
                "    svc_oof[val_i] = svc.predict_proba(X_val_f)\n",
                "    svc_test += svc.predict_proba(X_test_feat) / N_FOLDS\n",
                "    print(f\"  SVC: {svc.score(X_val_f, y_train[val_i]):.4f}\")\n",
                "\n",
                "    print(f\"  Time: {(time.time()-t1)/60:.1f}min\")\n",
                "\n",
                "print(f\"\\nTotal: {(time.time()-t0)/60:.1f}min\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Individual CV Accuracy:\n",
                        "  CNN: 0.4880\n",
                        "  ET:  0.5056\n",
                        "  RF:  0.5028\n",
                        "  HGB: 0.4944\n",
                        "  SVC: 0.5139\n",
                        "\n",
                        "Best weights: CNN=0.43, ET=0.25, RF=0.20, HGB=0.04, SVC=0.08\n",
                        "Ensemble CV: 0.5380\n"
                    ]
                }
            ],
            "source": [
                "# Cell 7: Ensemble Weight Search\n",
                "print(\"Individual CV Accuracy:\")\n",
                "print(f\"  CNN: {accuracy_score(y_train, cnn_oof.argmax(1)):.4f}\")\n",
                "print(f\"  ET:  {accuracy_score(y_train, et_oof.argmax(1)):.4f}\")\n",
                "print(f\"  RF:  {accuracy_score(y_train, rf_oof.argmax(1)):.4f}\")\n",
                "print(f\"  HGB: {accuracy_score(y_train, hgb_oof.argmax(1)):.4f}\")\n",
                "print(f\"  SVC: {accuracy_score(y_train, svc_oof.argmax(1)):.4f}\")\n",
                "\n",
                "# Random Search for 5 weights\n",
                "best_acc, best_w = 0, None\n",
                "for _ in range(2000):\n",
                "    w = np.random.dirichlet(np.ones(5))\n",
                "    oof = w[0]*cnn_oof + w[1]*et_oof + w[2]*rf_oof + w[3]*hgb_oof + w[4]*svc_oof\n",
                "    acc = accuracy_score(y_train, oof.argmax(1))\n",
                "    if acc > best_acc:\n",
                "        best_acc, best_w = acc, w\n",
                "\n",
                "print(f\"\\nBest weights: CNN={best_w[0]:.2f}, ET={best_w[1]:.2f}, RF={best_w[2]:.2f}, HGB={best_w[3]:.2f}, SVC={best_w[4]:.2f}\")\n",
                "print(f\"Ensemble CV: {best_acc:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Saved: submission_milestone2_v4.3.csv\n",
                        "\n",
                        "Distribution:\n",
                        "Label\n",
                        "0    227\n",
                        "1     51\n",
                        "2     49\n",
                        "3     72\n",
                        "4      1\n",
                        "Name: count, dtype: int64\n",
                        "\n",
                        "Total time: 13.8min\n"
                    ]
                }
            ],
            "source": [
                "# Cell 8: Submission\n",
                "w = best_w\n",
                "final = w[0]*cnn_test + w[1]*et_test + w[2]*rf_test + w[3]*hgb_test + w[4]*svc_test\n",
                "preds = final.argmax(1)\n",
                "\n",
                "submission = pd.DataFrame({'ID': np.arange(1, n_test + 1), 'Label': preds})\n",
                "submission.to_csv('submission_milestone2_v4.3.csv', index=False)\n",
                "\n",
                "print(\"Saved: submission_milestone2_v4.3.csv\")\n",
                "print(f\"\\nDistribution:\\n{submission['Label'].value_counts().sort_index()}\")\n",
                "print(f\"\\nTotal time: {(time.time()-t0)/60:.1f}min\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.14.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
