{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# IFT3395 Competition 2 - Winning Solution (Ensemble)\n",
        "**Target**: Score > 0.53\n",
        "\n",
        "## Strategy\n",
        "1. **Hybrid Ensemble**: Combines a custom **Lightweight CNN** (PyTorch) with **ExtraTrees** (Scikit-Learn).\n",
        "2. **Class Imbalance Handling**: Uses `WeightedRandomSampler` to ensure the model learns rare classes (Class 4).\n",
        "3. **Training from Scratch**: No pre-trained models used, complying with strict competition rules.\n",
        "4. **Test Time Augmentation (TTA)**: Improves robustness.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from torchvision import transforms\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tqdm.notebook import tqdm\n",
        "from PIL import Image\n",
        "\n",
        "# Set Seeds for Reproducibility\n",
        "SEED = 42\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(SEED)\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Data\n",
        "def load_data():\n",
        "    # Attempt to load from current directory 'data' folder\n",
        "    if os.path.exists('data/train_data.pkl'):\n",
        "        path_prefix = 'data/'\n",
        "    else:\n",
        "        # Fallback just in case\n",
        "        path_prefix = ''\n",
        "        \n",
        "    with open(f'{path_prefix}train_data.pkl', 'rb') as f:\n",
        "        train_data = pickle.load(f)\n",
        "    with open(f'{path_prefix}test_data.pkl', 'rb') as f:\n",
        "        test_data = pickle.load(f)\n",
        "    return train_data, test_data\n",
        "\n",
        "train_data, test_data = load_data()\n",
        "train_images = train_data['images']  # (1080, 28, 28, 3)\n",
        "train_labels = train_data['labels'].flatten()\n",
        "test_images = test_data['images']    # (400, 28, 28, 3)\n",
        "\n",
        "# Check data stats\n",
        "print(f\"Train Img: {train_images.shape}, Labels: {train_labels.shape}\")\n",
        "print(f\"Test Img: {test_images.shape}\")\n",
        "print(f\"Classes: {np.unique(train_labels)}\")\n",
        "class_counts = np.bincount(train_labels)\n",
        "print(f\"Class Counts: {class_counts}\")\n",
        "\n",
        "# Calculate Class Weights for Loss Function (Inverse Frequency)\n",
        "# We want to penalize mistakes on rare classes more\n",
        "total_samples = len(train_labels)\n",
        "class_weights = total_samples / (len(class_counts) * class_counts)\n",
        "class_weights_tensor = torch.FloatTensor(class_weights).to(DEVICE)\n",
        "print(f\"Class Weights: {class_weights}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class QuickDrawDataset(Dataset):\n",
        "    def __init__(self, images, labels=None, transform=None):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        img = self.images[idx]\n",
        "        \n",
        "        # Determine format and convert to PIL\n",
        "        if img.dtype != np.uint8:\n",
        "             if img.max() <= 1.0:\n",
        "                 img = (img * 255).astype(np.uint8)\n",
        "             else:\n",
        "                 img = img.astype(np.uint8)\n",
        "        \n",
        "        # Convert to PIL for Transforms\n",
        "        img_pil = Image.fromarray(img)\n",
        "        \n",
        "        if self.transform:\n",
        "            img_tensor = self.transform(img_pil)\n",
        "        else:\n",
        "            img_tensor = transforms.ToTensor()(img_pil)\n",
        "            \n",
        "        if self.labels is not None:\n",
        "            return img_tensor, self.labels[idx]\n",
        "        return img_tensor\n",
        "\n",
        "# Strong Augmentation for Training\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Validation/Test Transforms\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, \n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, \n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, \n",
        "                          stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class SmallResNet(nn.Module):\n",
        "    def __init__(self, num_classes=5):\n",
        "        super(SmallResNet, self).__init__()\n",
        "        # Initial: 3 -> 64\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        \n",
        "        # 3 Stages of ResBlocks\n",
        "        self.layer1 = ResidualBlock(64, 64, stride=1)\n",
        "        self.layer2 = ResidualBlock(64, 128, stride=2) # 28 -> 14\n",
        "        self.layer3 = ResidualBlock(128, 256, stride=2) # 14 -> 7\n",
        "        self.layer4 = ResidualBlock(256, 512, stride=2) # 7 -> 3\n",
        "        \n",
        "        # Classifier\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "model_test = SmallResNet()\n",
        "print(f\"Model Parameters: {count_parameters(model_test):,}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_one_fold(fold_idx, train_idx, val_idx, X, y):\n",
        "    print(f\"\\n{'='*20} Fold {fold_idx+1} {'='*20}\")\n",
        "    \n",
        "    # Prepare Data\n",
        "    train_ds = QuickDrawDataset(X[train_idx], y[train_idx], transform=train_transforms)\n",
        "    val_ds = QuickDrawDataset(X[val_idx], y[val_idx], transform=val_transforms)\n",
        "    \n",
        "    # Weighted Sampler for Imbalance\n",
        "    y_train = y[train_idx]\n",
        "    count = np.bincount(y_train)\n",
        "    weight_per_class = 1. / torch.tensor(count, dtype=torch.float)\n",
        "    samples_weight = np.array([weight_per_class[t] for t in y_train])\n",
        "    samples_weight = torch.from_numpy(samples_weight)\n",
        "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
        "    \n",
        "    train_loader = DataLoader(train_ds, batch_size=64, sampler=sampler, num_workers=0)\n",
        "    val_loader = DataLoader(val_ds, batch_size=64, shuffle=False, num_workers=0)\n",
        "    \n",
        "    # Init Model\n",
        "    model = SmallResNet(num_classes=5).to(DEVICE)\n",
        "    \n",
        "    # Optimizer & Scheduler\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=40)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    best_acc = 0.0\n",
        "    best_model = None\n",
        "    \n",
        "    EPOCHS = 40\n",
        "    \n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for imgs, labels in train_loader:\n",
        "            imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(imgs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "            \n",
        "        # Validation\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for imgs, labels in val_loader:\n",
        "                imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
        "                outputs = model(imgs)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "        \n",
        "        acc = correct / total\n",
        "        scheduler.step()\n",
        "        \n",
        "        if acc > best_acc:\n",
        "            best_acc = acc\n",
        "            best_model = model.state_dict().copy() # Copy essential!\n",
        "            \n",
        "    print(f\"Fold {fold_idx+1} Best Val Acc: {best_acc:.4f}\")\n",
        "    \n",
        "    # Restore best model\n",
        "    model.load_state_dict(best_model)\n",
        "    return model, best_acc\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_sklearn_fold(train_idx, val_idx, X_flat, y):\n",
        "    X_tr, y_tr = X_flat[train_idx], y[train_idx]\n",
        "    X_val, y_val = X_flat[val_idx], y[val_idx]\n",
        "    \n",
        "    # Use ExtraTrees\n",
        "    clf = ExtraTreesClassifier(n_estimators=500, n_jobs=-1, random_state=SEED, \n",
        "                               class_weight='balanced')\n",
        "    clf.fit(X_tr, y_tr)\n",
        "    \n",
        "    acc = clf.score(X_val, y_val)\n",
        "    print(f\"Sklearn ExtraTrees Val Acc: {acc:.4f}\")\n",
        "    return clf, acc\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Flatten data for Sklearn\n",
        "X_flat = train_images.reshape(len(train_images), -1).astype(np.float32) / 255.0\n",
        "\n",
        "# 5-Fold Stratified CV\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
        "\n",
        "pytorch_models = []\n",
        "sklearn_models = []\n",
        "val_accuracies = []\n",
        "\n",
        "for fold_i, (train_idx, val_idx) in enumerate(skf.split(train_images, train_labels)):\n",
        "    \n",
        "    # 1. Train PyTorch Model\n",
        "    pt_model, pt_acc = train_one_fold(fold_i, train_idx, val_idx, train_images, train_labels)\n",
        "    pytorch_models.append(pt_model)\n",
        "    \n",
        "    # 2. Train Sklearn Model\n",
        "    sk_model, sk_acc = train_sklearn_fold(train_idx, val_idx, X_flat, train_labels)\n",
        "    sklearn_models.append(sk_model)\n",
        "    \n",
        "    val_accuracies.append((pt_acc + sk_acc)/2)\n",
        "\n",
        "print(f\"\\nAverage Estimated CV Score: {np.mean(val_accuracies):.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inference Logic\n",
        "def predict_test(pt_models, sk_models, test_imgs):\n",
        "    # Prepare PyTorch Test Loader\n",
        "    test_ds = QuickDrawDataset(test_imgs, transform=val_transforms)\n",
        "    test_loader = DataLoader(test_ds, batch_size=64, shuffle=False)\n",
        "    \n",
        "    # Prepare Sklearn Data\n",
        "    X_test_flat = test_imgs.reshape(len(test_imgs), -1).astype(np.float32) / 255.0\n",
        "    \n",
        "    # Arrays to store probabilities\n",
        "    pt_probs = np.zeros((len(test_imgs), 5)) \n",
        "    sk_probs = np.zeros((len(test_imgs), 5))\n",
        "    \n",
        "    print(\"Predicting with PyTorch Models...\")\n",
        "    for model in pt_models:\n",
        "        model.eval()\n",
        "        fold_probs = []\n",
        "        with torch.no_grad():\n",
        "            for imgs in test_loader:\n",
        "                imgs = imgs.to(DEVICE)\n",
        "                output = model(imgs)\n",
        "                # TTA Integration (Simple Flip)\n",
        "                # Let's do a simple TTA here: original + h_flip\n",
        "                \n",
        "                # Forward Pass (Original)\n",
        "                out1 = model(imgs)\n",
        "                prob1 = F.softmax(out1, dim=1)\n",
        "                \n",
        "                # Forward Pass (H Flip)\n",
        "                imgs_flip = torch.flip(imgs, [3]) # N,C,H,W\n",
        "                out2 = model(imgs_flip)\n",
        "                prob2 = F.softmax(out2, dim=1)\n",
        "                \n",
        "                # Average\n",
        "                prob = (prob1 + prob2) / 2\n",
        "                fold_probs.append(prob.cpu().numpy())\n",
        "                \n",
        "        pt_probs += np.concatenate(fold_probs, axis=0)\n",
        "    pt_probs /= len(pt_models)\n",
        "    \n",
        "    print(\"Predicting with Sklearn Models...\")\n",
        "    for clf in sk_models:\n",
        "        sk_probs += clf.predict_proba(X_test_flat)\n",
        "    sk_probs /= len(sk_models)\n",
        "    \n",
        "    # Weighted Ensemble (50/50)\n",
        "    final_probs = 0.5 * pt_probs + 0.5 * sk_probs\n",
        "    predictions = np.argmax(final_probs, axis=1)\n",
        "    \n",
        "    return predictions\n",
        "\n",
        "predictions = predict_test(pytorch_models, sklearn_models, test_images)\n",
        "print(f\"Predictions check: {predictions[:10]}\")\n",
        "print(f\"Class Distribution: {np.bincount(predictions)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate Submission\n",
        "submission_df = pd.DataFrame({\n",
        "    'Id': np.arange(len(predictions)),\n",
        "    'Category': predictions\n",
        "})\n",
        "\n",
        "filename = 'submission_winning_solution.csv'\n",
        "submission_df.to_csv(filename, index=False)\n",
        "print(f\"Saved submission to {filename}\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}