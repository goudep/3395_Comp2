# IFT3395 Competition 2 - 项目报告

## 项目信息

- **项目名称**：糖尿病视网膜病变严重程度分类
- **任务类型**：多分类图像分类（5 个类别）
- **数据集规模**：训练集 1080 样本，测试集 400 样本

---

## 1. 问题分析与数据探索

### 1.1 数据集特征

- **图像尺寸**：28×28×3（RGB 彩色图像）
- **类别数量**：5 个（严重程度 0-4）
- **类别分布**：
  - 类别 0：486 样本（45.0%）
  - 类别 1：128 样本（11.9%）
  - 类别 2：206 样本（19.1%）
  - 类别 3：194 样本（18.0%）
  - 类别 4：66 样本（6.1%）

### 1.2 主要挑战

1. **类别严重不平衡**：类别 0 的样本数是类别 4 的 7.4 倍
2. **小数据集**：仅 1080 个训练样本，容易过拟合
3. **实现限制**：只能使用 NumPy，无法使用成熟的 ML 库

### 1.3 解决方案选择理由

经过多次实验，我们发现：
- **深层网络（3+ 层）表现不佳**：验证准确率低于 0.5，可能由于梯度问题
- **简单逻辑回归**：验证准确率约 0.46-0.47，接近但未超过 baseline
- **宽 2 层 MLP + 正则化 + 集成**：最终达到验证准确率 0.50+，成功超越 baseline

---

## 2. 方法设计

### 2.1 数据预处理

#### 2.1.1 图像展平与归一化

**方法**：将 28×28×3 的图像展平为 2352 维向量，并归一化到 [0, 1]

**理由**：
- MLP 需要一维输入
- 归一化到 [0, 1] 便于后续标准化处理
- 保持像素值的数值范围一致

#### 2.1.2 标准化（Z-score Normalization）

**方法**：计算训练集的均值和标准差，将数据转换为均值 0、标准差 1

**理由**：
- 标准化后的数据有助于梯度下降收敛
- 不同特征尺度统一，避免某些特征主导训练
- 提高数值稳定性

### 2.2 类别不平衡处理

#### 2.2.1 类别权重计算

**方法**：
```python
class_weights_base = total_samples / class_counts
class_weights = class_weights_base ** 1.2  # 指数缩放
class_weights = class_weights / class_weights.mean()  # 归一化
```

**理由**：
- 使用逆频率作为基础权重，给少数类更高权重
- 指数 1.2 次方：比直接使用逆频率更平滑，避免过度关注少数类导致过拟合
- 归一化：保持权重均值为 1，维持数值稳定性

**效果**：类别 4（最少）权重为 2.4，类别 0（最多）权重为 0.22

### 2.3 模型架构设计

#### 2.3.1 网络结构选择

**最终选择**：2 层 MLP（输入层 → 隐藏层 → 输出层）

**架构细节**：
- 输入维度：2352（28×28×3）
- 隐藏层维度：512-896（通过超参搜索确定）
- 输出维度：5（类别数）
- 激活函数：ReLU（隐藏层）+ Softmax（输出层）

**选择理由**：
1. **避免梯度问题**：2 层网络避免了深层网络的梯度消失/爆炸
2. **足够的表达能力**：宽隐藏层（512-896 维）足以学习复杂模式
3. **训练稳定性**：2 层网络更容易收敛，训练过程更稳定
4. **实验验证**：3 层网络在验证集上表现更差（val_acc < 0.5）

#### 2.3.2 权重初始化

**方法**：He 初始化
```python
scale = sqrt(2.0 / input_dim)
weights = normal(0, scale)
```

**理由**：
- He 初始化专门为 ReLU 激活函数设计
- 确保前向传播时激活值的方差保持稳定
- 有助于梯度反向传播，避免梯度消失

#### 2.3.3 正则化策略

**Dropout**：
- 范围：0.1-0.2（通过超参搜索确定）
- 应用位置：输入层和隐藏层
- 理由：防止过拟合，提高泛化能力

**L2 正则化**：
- 范围：5e-5 到 9e-5
- 理由：限制权重过大，防止过拟合，同时不过度抑制模型容量

**梯度裁剪**：
- 阈值：5.0
- 理由：防止梯度爆炸，提高训练稳定性

### 2.4 训练策略

#### 2.4.1 损失函数

**方法**：加权交叉熵损失
```python
loss = -sum(class_weights[y] * log(probs[y])) / sum(class_weights[y])
loss += 0.5 * l2 * sum(weights^2)  # L2 正则化项
```

**理由**：
- 加权交叉熵：处理类别不平衡
- L2 正则化：防止过拟合

#### 2.4.2 优化器

**方法**：小批量梯度下降（Mini-batch Gradient Descent）

**超参数**：
- 学习率：0.045-0.055（通过搜索确定）
- 批大小：96-128
- 学习率调度：余弦退火

**学习率调度**：
```python
lr = base_lr * 0.5 * (1 + cos(π * epoch / total_epochs))
```

**理由**：
- 余弦退火：学习率从初始值平滑降至 0，有助于精细调优
- 小批量：平衡训练速度和梯度估计稳定性

#### 2.4.3 早停（Early Stopping）

**方法**：监控验证集准确率，若连续 N 个 epoch 未提升则停止

**参数**：
- Patience：40-65（根据配置不同）
- 阈值：1e-4（验证准确率提升的最小值）

**理由**：
- 防止过拟合
- 自动选择最佳模型（保存验证集表现最好的参数）

### 2.5 数据增强

#### 2.5.1 训练时增强

**方法**：
1. 随机水平翻转（概率 0.5）
2. 随机垂直翻转（概率 0.25）
3. 添加高斯噪声（标准差 0.02）
4. 随机亮度调整（范围 ±0.1）

**理由**：
- 增加训练数据的多样性
- 提高模型对图像变换的鲁棒性
- 缓解小数据集问题

**实现细节**：
- 在每次 mini-batch 训练时动态生成增强数据
- 增强后的数据需要重新标准化

#### 2.5.2 测试时增强（TTA）

**方法**：对测试图像应用 5 种变换，分别预测后平均概率

**变换模式**：
1. 原始图像（identity）
2. 水平翻转（hflip）
3. 垂直翻转（vflip）
4. 亮度增加（bright+）
5. 亮度减少（bright-）

**理由**：
- 提高预测稳定性
- 减少单次预测的随机性
- 通常能提升 1-2% 的准确率

### 2.6 超参数搜索

#### 2.6.1 搜索空间

测试了 5 种配置组合：

| 配置 | hidden_dim | lr | l2 | dropout | epochs | batch_size | patience |
|------|------------|----|----|---------|--------|------------|----------|
| 1 | 512 | 0.05 | 6e-5 | 0.15 | 350 | 112 | 50 |
| 2 | 640 | 0.055 | 8e-5 | 0.18 | 380 | 96 | 55 |
| 3 | 768 | 0.048 | 7e-5 | 0.12 | 400 | 128 | 60 |
| 4 | 896 | 0.052 | 9e-5 | 0.20 | 360 | 104 | 50 |
| 5 | 640 | 0.045 | 5e-5 | 0.10 | 420 | 120 | 65 |

#### 2.6.2 搜索结果

最佳配置：配置 3（hidden_dim=768, lr=0.048, l2=7e-5, dropout=0.12）
- 验证准确率：0.5046
- 训练准确率：0.6273

**选择理由**：
- 验证准确率最高
- 训练准确率与验证准确率差距适中（未严重过拟合）

### 2.7 模型集成

#### 2.7.1 集成策略

**方法**：集成 top-3 模型进行软投票

**步骤**：
1. 选择验证集表现最好的 3 个配置
2. 在全部训练数据上重新训练这 3 个模型
3. 对测试集，每个模型对每种 TTA 模式预测
4. 对所有预测概率求平均，得到最终预测

**理由**：
- 不同配置的模型可能学到不同的特征
- 集成可以降低方差，提高泛化能力
- 软投票（平均概率）比硬投票（多数表决）更稳定

---

## 3. 实现细节

### 3.1 核心类：RobustMLP

#### 3.1.1 前向传播

```python
def _forward(self, x, train=False):
    # 输入层 Dropout
    if train and self.dropout > 0:
        x = x * dropout_mask
    
    # 第一层：输入 → 隐藏层
    z1 = x @ w1 + b1
    h1 = ReLU(z1)  # ReLU 激活
    
    # 隐藏层 Dropout
    if train and self.dropout > 0:
        h1 = h1 * dropout_mask
    
    # 第二层：隐藏层 → 输出层
    logits = h1 @ w2 + b2
    return logits
```

#### 3.1.2 反向传播

使用链式法则计算梯度：
1. 计算输出层梯度
2. 通过 ReLU 的梯度（z1 <= 0 时梯度为 0）
3. 计算隐藏层梯度
4. 应用梯度裁剪
5. 更新权重和偏置

#### 3.1.3 Softmax 实现

```python
def _softmax(self, logits):
    # 数值稳定性：减去最大值
    logits = logits - logits.max(axis=1, keepdims=True)
    exp = np.exp(logits)
    return exp / exp.sum(axis=1, keepdims=True)
```

**数值稳定性处理**：减去最大值避免 exp 溢出

---

## 4. 实验结果

### 4.1 超参搜索结果

| 配置 | 训练准确率 | 验证准确率 |
|------|-----------|-----------|
| 1 (512, 0.05) | 0.5914 | **0.5000** |
| 2 (640, 0.055) | 0.5856 | 0.4769 |
| 3 (768, 0.048) | 0.6273 | **0.5046** |
| 4 (896, 0.052) | 0.5289 | 0.4444 |
| 5 (640, 0.045) | 0.5995 | 0.4676 |

**最佳配置**：配置 3（hidden_dim=768）

### 4.2 最终模型表现

- **验证准确率**：0.5046（超过 baseline 0.455）
- **集成模型**：top-3 模型 + TTA
- **Kaggle 提交**：`submission_robust_wide.csv`

---

## 5. 方法选择理由总结

### 5.1 为什么选择 2 层 MLP？

1. **实验验证**：3 层网络验证准确率更低（< 0.5）
2. **梯度稳定性**：2 层网络梯度传播更稳定
3. **足够容量**：宽隐藏层（768 维）足以学习复杂模式

### 5.2 为什么使用这些正则化参数？

1. **Dropout 0.12**：平衡过拟合和欠拟合
2. **L2 7e-5**：适中的正则化强度
3. **梯度裁剪 5.0**：防止训练不稳定

### 5.3 为什么集成 top-3 而不是更多？

1. **计算成本**：3 个模型已经提供足够的多样性
2. **收益递减**：更多模型提升有限
3. **验证集表现**：top-3 的验证准确率都超过 0.47

### 5.4 为什么使用 TTA？

1. **提升稳定性**：平均多个预测减少随机性
2. **简单有效**：实现简单，通常能提升 1-2%
3. **无额外训练成本**：只需在推理时应用

---

## 6. 代码组织

### 6.1 文件结构

```
IFT3395_Competition2/
├── data/
│   ├── train_data.pkl
│   └── test_data.pkl
├── ift3395_robust_wide.ipynb  # 主 notebook（带详细注释）
└── submission_robust_wide.csv  # Kaggle 提交文件
```

### 6.2 主要组件

1. **数据加载与预处理**：`load_split()`, `flatten_and_normalize()`, `StandardScaler`
2. **数据增强**：`augment_batch()`, `apply_tta()`
3. **模型**：`RobustMLP` 类
4. **训练流程**：超参搜索 → 选择最佳 → 全量训练 → 集成预测

---

## 7. 总结

### 7.1 成功因素

1. **稳健的架构**：2 层宽网络避免了深层网络的问题
2. **平衡的正则化**：适中的 dropout 和 L2 防止过拟合
3. **类别权重**：有效处理类别不平衡
4. **数据增强**：提高泛化能力
5. **模型集成**：进一步提升性能

### 7.2 关键经验

1. **简单往往更好**：2 层网络比 3 层表现更好
2. **正则化要平衡**：过强的正则化会导致欠拟合
3. **超参搜索很重要**：不同配置的验证准确率差异可达 6%
4. **集成有效**：top-3 模型集成比单模型更稳定

### 7.3 未来改进方向

1. **更精细的超参搜索**：扩大搜索空间
2. **交叉验证**：使用 5 折交叉验证更稳健
3. **特征工程**：尝试 PCA 降维或其他特征提取
4. **更多数据增强**：旋转、缩放等（需注意医学图像的合理性）

---

## 8. 参考文献

- He, K., et al. (2015). "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification." *ICCV 2015*.
- Srivastava, N., et al. (2014). "Dropout: A Simple Way to Prevent Neural Networks from Overfitting." *JMLR 2014*.
- Kingma, D. P., & Ba, J. (2014). "Adam: A Method for Stochastic Optimization." *ICLR 2015*.

---

**报告生成日期**：2025年1月
**最终 Kaggle 分数**：超过 baseline 0.455 ✓

