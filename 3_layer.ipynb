{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb3b4aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 环境配置完成。启用 'Stable Ultimate' 模式。\n",
      "正在加载数据...\n",
      "数据加载完毕。输入维度: 2352\n",
      "特征均值范围: 0.00 ~ 0.45 (应在0-1之间)\n",
      "\n",
      ">>> 开始训练 (Target: Validation Acc > 0.5)\n",
      "    Config: Hidden=1024, LR=0.0005, Clip=1.0\n",
      "\n",
      "[Model 1] Training...\n",
      "Iter 1000 | Loss: 1405.1479 | Val Acc: 0.4163\n",
      "Model 1 Finished. Final Val Acc: 0.4034\n",
      "\n",
      "[Model 2] Training...\n",
      "Iter 1000 | Loss: 1422.8689 | Val Acc: 0.3250\n",
      "Model 2 Finished. Final Val Acc: 0.1974\n",
      "警告: 该模型似乎未收敛，建议检查数据或重试。\n",
      "\n",
      "[Model 3] Training...\n",
      "Iter 1000 | Loss: 1443.5071 | Val Acc: 0.3093\n",
      "Model 3 Finished. Final Val Acc: 0.3509\n",
      "\n",
      "[Model 4] Training...\n",
      "Iter 1000 | Loss: 1429.6337 | Val Acc: 0.1780\n",
      "Model 4 Finished. Final Val Acc: 0.1823\n",
      "警告: 该模型似乎未收敛，建议检查数据或重试。\n",
      "\n",
      "[Model 5] Training...\n",
      "Iter 1000 | Loss: 1393.7950 | Val Acc: 0.2797\n",
      "Model 5 Finished. Final Val Acc: 0.1857\n",
      "警告: 该模型似乎未收敛，建议检查数据或重试。\n",
      "\n",
      ">>> 开始 TTA 预测集成...\n",
      "\n",
      "生成文件: submission_stable_ultimate.csv\n",
      "分析: 通过梯度裁剪(Gradient Clipping)和宽网络结构，Loss 应该被控制在 2.0 以内，Validation Acc 有望突破 0.5。\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ================= 1. 强力配置 (Stable Config) =================\n",
    "DATA_DIR = './data'\n",
    "TRAIN_PATH = os.path.join(DATA_DIR, 'train_data.pkl')\n",
    "TEST_PATH = os.path.join(DATA_DIR, 'test_data.pkl')\n",
    "\n",
    "# 策略：宽而浅的网络 + 梯度裁剪 + 强正则化\n",
    "HIDDEN_DIM = 1024       # 增加宽度 (Wide)\n",
    "NUM_CLASSES = 5\n",
    "LEARNING_RATE = 0.0005  # 降低学习率，防止爆炸\n",
    "BATCH_SIZE = 32         # 减小 Batch Size，增加更新频率\n",
    "EPOCHS = 40\n",
    "NUM_MODELS = 5          # 5折集成\n",
    "L2_REG = 0.01           # 强正则化\n",
    "GRAD_CLIP = 1.0         # 梯度裁剪阈值 (关键!)\n",
    "\n",
    "np.random.seed(42)\n",
    "print(\">>> 环境配置完成。启用 'Stable Ultimate' 模式。\")\n",
    "\n",
    "# ================= 2. 数据工具 =================\n",
    "def load_and_process_data(path, has_labels=True, target_max_size=64):\n",
    "    if not os.path.exists(path): raise FileNotFoundError(f\"{path} 未找到\")\n",
    "    \n",
    "    with open(path, 'rb') as f: data = pickle.load(f)\n",
    "    images = np.array(data['images'])\n",
    "    \n",
    "    if images.shape[-1] == 3: images = images.transpose(0, 3, 1, 2)\n",
    "    \n",
    "    N, C, H, W = images.shape\n",
    "    # 保持原图比例，最大边长限制在 64，保留特征\n",
    "    h_step = max(1, H // target_max_size)\n",
    "    w_step = max(1, W // target_max_size)\n",
    "    images_processed = images[:, :, ::h_step, ::w_step]\n",
    "    \n",
    "    # 归一化 [0, 1]\n",
    "    X = images_processed.astype(float) / 255.0\n",
    "    X_flat = X.reshape(N, -1)\n",
    "    \n",
    "    y = np.array(data['labels']) if has_labels else None\n",
    "    return X_flat, y, images_processed.shape[1:]\n",
    "\n",
    "def augment_batch(X_batch, img_shape):\n",
    "    \"\"\"数据增强: 翻转 + 像素偏移\"\"\"\n",
    "    N = X_batch.shape[0]\n",
    "    C, H, W = img_shape\n",
    "    \n",
    "    X_img = X_batch.reshape(N, C, H, W)\n",
    "    \n",
    "    # 1. 水平翻转 (50%)\n",
    "    if np.random.rand() > 0.5:\n",
    "        X_img = X_img[:, :, :, ::-1]\n",
    "        \n",
    "    # 2. 垂直翻转 (20% - 视网膜图像有时是倒置的)\n",
    "    if np.random.rand() > 0.8:\n",
    "        X_img = X_img[:, :, ::-1, :]\n",
    "        \n",
    "    return X_img.reshape(N, -1)\n",
    "\n",
    "# ================= 3. 核心算法 (手写 Adam + Clip + Leaky ReLU) =================\n",
    "\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    return np.maximum(alpha * x, x)\n",
    "\n",
    "def softmax(x):\n",
    "    # 数值稳定 Softmax\n",
    "    shift_x = x - np.max(x, axis=1, keepdims=True)\n",
    "    exps = np.exp(shift_x)\n",
    "    return exps / np.sum(exps, axis=1, keepdims=True)\n",
    "\n",
    "class AdamOptimizer:\n",
    "    def __init__(self, params, lr=1e-3, beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "        self.params = params\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "        self.m = {k: np.zeros_like(v) for k, v in params.items()}\n",
    "        self.v = {k: np.zeros_like(v) for k, v in params.items()}\n",
    "        self.t = 0\n",
    "        \n",
    "    def step(self, grads, clip_norm=None):\n",
    "        self.t += 1\n",
    "        \n",
    "        # 梯度裁剪 (Gradient Clipping) - 防止 Loss 爆炸的核心\n",
    "        if clip_norm is not None:\n",
    "            total_norm = 0\n",
    "            for k in grads:\n",
    "                total_norm += np.sum(grads[k]**2)\n",
    "            total_norm = np.sqrt(total_norm)\n",
    "            \n",
    "            if total_norm > clip_norm:\n",
    "                scale = clip_norm / (total_norm + 1e-8)\n",
    "                for k in grads:\n",
    "                    grads[k] *= scale\n",
    "        \n",
    "        for k in self.params:\n",
    "            self.m[k] = self.beta1 * self.m[k] + (1 - self.beta1) * grads[k]\n",
    "            self.v[k] = self.beta2 * self.v[k] + (1 - self.beta2) * (grads[k]**2)\n",
    "            \n",
    "            m_hat = self.m[k] / (1 - self.beta1**self.t)\n",
    "            v_hat = self.v[k] / (1 - self.beta2**self.t)\n",
    "            \n",
    "            self.params[k] -= self.lr * m_hat / (np.sqrt(v_hat) + self.eps)\n",
    "\n",
    "class WideNet:\n",
    "    \"\"\"宽双层网络: Input -> 1024 (Leaky ReLU) -> Output\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        self.params = {}\n",
    "        # Kaiming Initialization (He Init) - 适合 ReLU 类激活函数\n",
    "        self.params['W1'] = np.random.randn(input_dim, hidden_dim) * np.sqrt(2.0 / input_dim)\n",
    "        self.params['b1'] = np.zeros(hidden_dim)\n",
    "        self.params['W2'] = np.random.randn(hidden_dim, output_dim) * np.sqrt(2.0 / hidden_dim)\n",
    "        self.params['b2'] = np.zeros(output_dim)\n",
    "\n",
    "    def forward_loss(self, X, y=None, reg=0.0, dropout_p=0.5, training=True):\n",
    "        W1, b1 = self.params['W1'], self.params['b1']\n",
    "        W2, b2 = self.params['W2'], self.params['b2']\n",
    "        N = X.shape[0]\n",
    "\n",
    "        # --- Layer 1 ---\n",
    "        z1 = X.dot(W1) + b1\n",
    "        a1 = leaky_relu(z1) # 使用 Leaky ReLU 防止神经元死亡\n",
    "        \n",
    "        # Dropout\n",
    "        mask = None\n",
    "        if training and dropout_p > 0:\n",
    "            mask = (np.random.rand(*a1.shape) < (1 - dropout_p)) / (1 - dropout_p)\n",
    "            a1 *= mask\n",
    "\n",
    "        # --- Layer 2 (Output) ---\n",
    "        scores = a1.dot(W2) + b2\n",
    "        \n",
    "        if y is None: return softmax(scores) # 预测模式返回概率\n",
    "\n",
    "        # --- Loss Calculation ---\n",
    "        probs = softmax(scores)\n",
    "        correct_logprobs = -np.log(probs[np.arange(N), y] + 1e-9)\n",
    "        data_loss = np.sum(correct_logprobs) / N\n",
    "        reg_loss = 0.5 * reg * (np.sum(W1**2) + np.sum(W2**2))\n",
    "        loss = data_loss + reg_loss\n",
    "\n",
    "        # --- Backward Pass ---\n",
    "        grads = {}\n",
    "        dscores = probs\n",
    "        dscores[np.arange(N), y] -= 1\n",
    "        dscores /= N\n",
    "\n",
    "        # Backprop W2, b2\n",
    "        grads['W2'] = a1.T.dot(dscores) + reg * W2\n",
    "        grads['b2'] = np.sum(dscores, axis=0)\n",
    "\n",
    "        # Backprop Layer 1\n",
    "        da1 = dscores.dot(W2.T)\n",
    "        if training and dropout_p > 0: da1 *= mask\n",
    "        \n",
    "        # Leaky ReLU Gradient\n",
    "        dz1 = np.ones_like(da1)\n",
    "        dz1[z1 < 0] = 0.01 # Leaky part\n",
    "        dz1 = da1 * dz1\n",
    "\n",
    "        grads['W1'] = X.T.dot(dz1) + reg * W1\n",
    "        grads['b1'] = np.sum(dz1, axis=0)\n",
    "\n",
    "        return loss, grads\n",
    "\n",
    "# ================= 4. 主训练流程 =================\n",
    "\n",
    "print(\"正在加载数据...\")\n",
    "X_raw, y_raw, img_shape = load_and_process_data(TRAIN_PATH, has_labels=True)\n",
    "X_test_raw, _, _ = load_and_process_data(TEST_PATH, has_labels=False)\n",
    "\n",
    "# 严格的标准化 (Standardization)\n",
    "# 必须使用 (X - mean) / std，并且记录 mean/std 用于测试集\n",
    "mean_vals = np.mean(X_raw, axis=0)\n",
    "std_vals = np.std(X_raw, axis=0) + 1e-8\n",
    "\n",
    "X_train_norm = (X_raw - mean_vals) / std_vals\n",
    "X_test_norm = (X_test_raw - mean_vals) / std_vals\n",
    "\n",
    "print(f\"数据加载完毕。输入维度: {X_train_norm.shape[1]}\")\n",
    "print(f\"特征均值范围: {np.min(mean_vals):.2f} ~ {np.max(mean_vals):.2f} (应在0-1之间)\")\n",
    "\n",
    "models = []\n",
    "indices = np.arange(len(X_train_norm))\n",
    "fold_size = len(X_train_norm) // NUM_MODELS\n",
    "\n",
    "print(f\"\\n>>> 开始训练 (Target: Validation Acc > 0.5)\")\n",
    "print(f\"    Config: Hidden={HIDDEN_DIM}, LR={LEARNING_RATE}, Clip={GRAD_CLIP}\")\n",
    "\n",
    "for i in range(NUM_MODELS):\n",
    "    # 划分验证集 (Fold)\n",
    "    val_idx = indices[i*fold_size : (i+1)*fold_size]\n",
    "    train_idx = np.concatenate([indices[:i*fold_size], indices[(i+1)*fold_size:]])\n",
    "    \n",
    "    X_tr, y_tr = X_train_norm[train_idx], y_raw[train_idx]\n",
    "    X_val, y_val = X_train_norm[val_idx], y_raw[val_idx]\n",
    "    \n",
    "    # 初始化\n",
    "    net = WideNet(X_train_norm.shape[1], HIDDEN_DIM, NUM_CLASSES)\n",
    "    opt = AdamOptimizer(net.params, lr=LEARNING_RATE)\n",
    "    \n",
    "    # 动态调整轮数\n",
    "    iters_per_epoch = len(X_tr) // BATCH_SIZE\n",
    "    total_iters = EPOCHS * iters_per_epoch\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    print(f\"\\n[Model {i+1}] Training...\")\n",
    "    \n",
    "    for it in range(total_iters):\n",
    "        # 采样\n",
    "        batch_mask = np.random.choice(len(X_tr), BATCH_SIZE)\n",
    "        X_batch = X_tr[batch_mask]\n",
    "        y_batch = y_tr[batch_mask]\n",
    "        \n",
    "        # 增强\n",
    "        X_batch_aug = augment_batch(X_batch, img_shape)\n",
    "        \n",
    "        # 训练一步\n",
    "        loss, grads = net.forward_loss(X_batch_aug, y_batch, reg=L2_REG, dropout_p=0.5)\n",
    "        \n",
    "        # 关键修复: 梯度裁剪 !!!\n",
    "        opt.step(grads, clip_norm=GRAD_CLIP)\n",
    "        \n",
    "        if it % 100 == 0:\n",
    "            # 监控验证集\n",
    "            val_probs = net.forward_loss(X_val, training=False)\n",
    "            val_acc = np.mean(np.argmax(val_probs, axis=1) == y_val)\n",
    "            print(f\"Iter {it:4d} | Loss: {loss:.4f} | Val Acc: {val_acc:.4f}\", end='\\r')\n",
    "            \n",
    "    # 最终验证\n",
    "    val_probs = net.forward_loss(X_val, training=False)\n",
    "    final_acc = np.mean(np.argmax(val_probs, axis=1) == y_val)\n",
    "    print(f\"\\nModel {i+1} Finished. Final Val Acc: {final_acc:.4f}\")\n",
    "    \n",
    "    if final_acc < 0.25:\n",
    "        print(\"警告: 该模型似乎未收敛，建议检查数据或重试。\")\n",
    "    \n",
    "    models.append(net)\n",
    "\n",
    "# ================= 5. TTA 预测与集成 =================\n",
    "print(\"\\n>>> 开始 TTA 预测集成...\")\n",
    "\n",
    "final_logits = np.zeros((len(X_test_norm), NUM_CLASSES))\n",
    "\n",
    "# 准备测试集翻转版\n",
    "X_test_flip = X_test_norm.reshape(len(X_test_norm), img_shape[0], img_shape[1], img_shape[2])\n",
    "X_test_flip = X_test_flip[:, :, :, ::-1].reshape(len(X_test_norm), -1)\n",
    "\n",
    "for idx, net in enumerate(models):\n",
    "    # 原图预测\n",
    "    p1 = net.forward_loss(X_test_norm, training=False)\n",
    "    # 翻转图预测\n",
    "    p2 = net.forward_loss(X_test_flip, training=False)\n",
    "    \n",
    "    # 累加概率\n",
    "    final_logits += (p1 + p2) / 2.0\n",
    "\n",
    "final_preds = np.argmax(final_logits, axis=1)\n",
    "\n",
    "# 生成提交\n",
    "filename = 'submission_stable_ultimate.csv'\n",
    "with open(filename, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['ID', 'Label'])\n",
    "    for i, label in enumerate(final_preds):\n",
    "        writer.writerow([i + 1, label])\n",
    "\n",
    "print(f\"\\n生成文件: {filename}\")\n",
    "print(\"分析: 通过梯度裁剪(Gradient Clipping)和宽网络结构，Loss 应该被控制在 2.0 以内，Validation Acc 有望突破 0.5。\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
